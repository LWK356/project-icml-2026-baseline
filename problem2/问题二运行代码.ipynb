{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15046dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Problem2] 开始处理任务: Conv_Case0\n",
      "[文件保存] 任务 Conv_Case0 输出完成：\n",
      "  - 调度序列：3039 条记录 → ./CSV版本\\Attachment\\Problem2\\Conv_Case0_schedule.csv\n",
      "  - 内存分配：817 条记录 → ./CSV版本\\Attachment\\Problem2\\Conv_Case0_memory.csv\n",
      "  - SPILL操作：473 条记录 → ./CSV版本\\Attachment\\Problem2\\Conv_Case0_spill.csv\n",
      "[Problem2] 完成任务: Conv_Case0 | SPILL次数: 236 | 内存记录数: 817 | 额外数据搬运: 57146\n",
      "\n",
      "[Problem2] 开始处理任务: Conv_Case1\n",
      "[文件保存] 任务 Conv_Case1 输出完成：\n",
      "  - 调度序列：47108 条记录 → ./CSV版本\\Attachment\\Problem2\\Conv_Case1_schedule.csv\n",
      "  - 内存分配：11386 条记录 → ./CSV版本\\Attachment\\Problem2\\Conv_Case1_memory.csv\n",
      "  - SPILL操作：11649 条记录 → ./CSV版本\\Attachment\\Problem2\\Conv_Case1_spill.csv\n",
      "[Problem2] 完成任务: Conv_Case1 | SPILL次数: 5824 | 内存记录数: 11386 | 额外数据搬运: 451707\n",
      "\n",
      "[Problem2] 开始处理任务: FlashAttention_Case0\n",
      "[文件保存] 任务 FlashAttention_Case0 输出完成：\n",
      "  - 调度序列：2105 条记录 → ./CSV版本\\Attachment\\Problem2\\FlashAttention_Case0_schedule.csv\n",
      "  - 内存分配：529 条记录 → ./CSV版本\\Attachment\\Problem2\\FlashAttention_Case0_memory.csv\n",
      "  - SPILL操作：432 条记录 → ./CSV版本\\Attachment\\Problem2\\FlashAttention_Case0_spill.csv\n",
      "[Problem2] 完成任务: FlashAttention_Case0 | SPILL次数: 216 | 内存记录数: 529 | 额外数据搬运: 35080\n",
      "\n",
      "[Problem2] 开始处理任务: FlashAttention_Case1\n",
      "[文件保存] 任务 FlashAttention_Case1 输出完成：\n",
      "  - 调度序列：8621 条记录 → ./CSV版本\\Attachment\\Problem2\\FlashAttention_Case1_schedule.csv\n",
      "  - 内存分配：2155 条记录 → ./CSV版本\\Attachment\\Problem2\\FlashAttention_Case1_memory.csv\n",
      "  - SPILL操作：1842 条记录 → ./CSV版本\\Attachment\\Problem2\\FlashAttention_Case1_spill.csv\n",
      "[Problem2] 完成任务: FlashAttention_Case1 | SPILL次数: 921 | 内存记录数: 2155 | 额外数据搬运: 148094\n",
      "\n",
      "[Problem2] 开始处理任务: Matmul_Case0\n",
      "[文件保存] 任务 Matmul_Case0 输出完成：\n",
      "  - 调度序列：4562 条记录 → ./CSV版本\\Attachment\\Problem2\\Matmul_Case0_schedule.csv\n",
      "  - 内存分配：874 条记录 → ./CSV版本\\Attachment\\Problem2\\Matmul_Case0_memory.csv\n",
      "  - SPILL操作：744 条记录 → ./CSV版本\\Attachment\\Problem2\\Matmul_Case0_spill.csv\n",
      "[Problem2] 完成任务: Matmul_Case0 | SPILL次数: 372 | 内存记录数: 874 | 额外数据搬运: 95232\n",
      "\n",
      "[Problem2] 开始处理任务: Matmul_Case1\n",
      "[文件保存] 任务 Matmul_Case1 输出完成：\n",
      "  - 调度序列：33954 条记录 → ./CSV版本\\Attachment\\Problem2\\Matmul_Case1_schedule.csv\n",
      "  - 内存分配：6222 条记录 → ./CSV版本\\Attachment\\Problem2\\Matmul_Case1_memory.csv\n",
      "  - SPILL操作：5716 条记录 → ./CSV版本\\Attachment\\Problem2\\Matmul_Case1_spill.csv\n",
      "[Problem2] 完成任务: Matmul_Case1 | SPILL次数: 2858 | 内存记录数: 6222 | 额外数据搬运: 731648\n",
      "\n",
      "[Problem2] 所有任务处理完毕。汇总信息已保存至 summary_metrics.json\n",
      "- Conv_Case0: SPILL=236, 内存记录数=817, ExtraData=57146\n",
      "- Conv_Case1: SPILL=5824, 内存记录数=11386, ExtraData=451707\n",
      "- FlashAttention_Case0: SPILL=216, 内存记录数=529, ExtraData=35080\n",
      "- FlashAttention_Case1: SPILL=921, 内存记录数=2155, ExtraData=148094\n",
      "- Matmul_Case0: SPILL=372, 内存记录数=874, ExtraData=95232\n",
      "- Matmul_Case1: SPILL=2858, 内存记录数=6222, ExtraData=731648\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import bisect\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = r\"./CSV版本\"\n",
    "PROBLEM1_DIR = os.path.join(DATA_DIR, \"Attachment\", \"Problem1\")\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"Attachment\", \"Problem2\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hardware cache capacity limits\n",
    "HARDWARE_CACHE_LIMITS = {\n",
    "    'L1': 4096,\n",
    "    'UB': 1024,\n",
    "    'L0A': 256,\n",
    "    'L0B': 256,\n",
    "    'L0C': 512,\n",
    "}\n",
    "\n",
    "# --- Data Structures and Core Algorithms (no functional changes) ---\n",
    "class MemoryManager:\n",
    "    def __init__(self, capacity: int, cache_type: str):\n",
    "        self.capacity = capacity\n",
    "        self.cache_type = cache_type\n",
    "        self.free_blocks: List[Tuple[int, int]] = [(0, capacity)]\n",
    "        self.allocated_blocks: Dict[int, Tuple[int, int]] = {}\n",
    "        self.last_access_time: Dict[int, int] = {}\n",
    "        self.timestamp = 0\n",
    "\n",
    "    def _get_next_timestamp(self) -> int:\n",
    "        self.timestamp += 1\n",
    "        return self.timestamp\n",
    "\n",
    "    def allocate(self, buf_id: int, size: int) -> Optional[int]:\n",
    "        if size > self.capacity:\n",
    "            return None\n",
    "        best_idx = -1\n",
    "        best_size = float('inf')\n",
    "        for i, (block_start, block_size) in enumerate(self.free_blocks):\n",
    "            if block_size >= size and block_size < best_size:\n",
    "                best_size = block_size\n",
    "                best_idx = i\n",
    "                if best_size == size:\n",
    "                    break\n",
    "        if best_idx != -1:\n",
    "            block_start, block_size = self.free_blocks.pop(best_idx)\n",
    "            self.allocated_blocks[buf_id] = (block_start, size)\n",
    "            self.last_access_time[buf_id] = self._get_next_timestamp()\n",
    "            remaining_size = block_size - size\n",
    "            if remaining_size > 0:\n",
    "                bisect.insort(self.free_blocks, (block_start + size, remaining_size))\n",
    "            return block_start\n",
    "        return None\n",
    "\n",
    "    def free(self, buf_id: int):\n",
    "        if buf_id not in self.allocated_blocks:\n",
    "            return\n",
    "        addr, size = self.allocated_blocks.pop(buf_id)\n",
    "        self.last_access_time.pop(buf_id, None)\n",
    "        new_free_block = (addr, size)\n",
    "        idx = bisect.bisect_left(self.free_blocks, new_free_block)\n",
    "        if idx > 0 and self.free_blocks[idx-1][0] + self.free_blocks[idx-1][1] == addr:\n",
    "            prev_addr, prev_size = self.free_blocks.pop(idx-1)\n",
    "            addr, size = prev_addr, prev_size + size\n",
    "            new_free_block = (addr, size)\n",
    "        if idx < len(self.free_blocks) and addr + size == self.free_blocks[idx][0]:\n",
    "            next_addr, next_size = self.free_blocks.pop(idx)\n",
    "            size += next_size\n",
    "            new_free_block = (addr, size)\n",
    "        bisect.insort(self.free_blocks, new_free_block)\n",
    "\n",
    "    def touch(self, buf_id: int):\n",
    "        if buf_id in self.allocated_blocks:\n",
    "            self.last_access_time[buf_id] = self._get_next_timestamp()\n",
    "\n",
    "    def select_victim_for_spill(self) -> Optional[int]:\n",
    "        if not self.allocated_blocks:\n",
    "            return None\n",
    "        return min(self.last_access_time.keys(), key=lambda k: self.last_access_time[k])\n",
    "    \n",
    "    def get_allocated_block_info(self, buf_id: int) -> Optional[Tuple[int, int]]:\n",
    "        return self.allocated_blocks.get(buf_id, None)\n",
    "    \n",
    "    def get_total_free_space(self) -> int:\n",
    "        return sum(size for _, size in self.free_blocks)\n",
    "    \n",
    "    def get_largest_free_block_size(self) -> int:\n",
    "        if not self.free_blocks:\n",
    "            return 0\n",
    "        return max(size for _, size in self.free_blocks)\n",
    "    \n",
    "    def clear_all_blocks(self) -> List[int]:\n",
    "        spilled_bufs = list(self.allocated_blocks.keys())\n",
    "        for buf_id in spilled_bufs:\n",
    "            self.free(buf_id)\n",
    "        return spilled_bufs\n",
    "    \n",
    "    def get_current_allocations(self) -> Dict[int, int]:\n",
    "        return {buf_id: addr for buf_id, (addr, _) in self.allocated_blocks.items()}\n",
    "\n",
    "# --- SPILL Node Generation (no changes) ---\n",
    "def create_spill_nodes(base_id: int, buf_id: int, size: int, cache_type: str) -> Tuple[dict, dict]:\n",
    "    spill_out_node = {\n",
    "        \"Id\": base_id,\n",
    "        \"Op\": \"SPILL_OUT\",\n",
    "        \"Pipe\": \"MTE3\",\n",
    "        \"Cycles\": size * 2 + 150,\n",
    "        \"Bufs\": [buf_id],\n",
    "        \"BufId\": buf_id,\n",
    "        \"Size\": size,\n",
    "        \"Type\": cache_type\n",
    "    }\n",
    "    spill_in_node = {\n",
    "        \"Id\": base_id + 1,\n",
    "        \"Op\": \"SPILL_IN\",\n",
    "        \"Pipe\": \"MTE2\",\n",
    "        \"Cycles\": size * 2 + 150,\n",
    "        \"Bufs\": [buf_id],\n",
    "        \"BufId\": buf_id,\n",
    "        \"Size\": size,\n",
    "        \"Type\": cache_type\n",
    "    }\n",
    "    return spill_out_node, spill_in_node\n",
    "\n",
    "# --- Main Scheduling and Allocation Logic (no functional changes) ---\n",
    "def process_schedule_with_memory_management(df_nodes: pd.DataFrame, schedule: List[int]) -> Tuple[List[int], Dict[int, int], List[str], int]:\n",
    "    node_index = {int(row['Id']): row for _, row in df_nodes.iterrows()}\n",
    "    buf_info_map: Dict[int, Tuple[int, str]] = {} \n",
    "    for _, row in df_nodes.iterrows():\n",
    "        if row['Op'] == 'ALLOC':\n",
    "            buf_id = int(row['BufId'])\n",
    "            size = int(row['Size']) if not pd.isna(row['Size']) else 0\n",
    "            cache_type = row['Type'] if not pd.isna(row['Type']) else 'L1'\n",
    "            buf_info_map[buf_id] = (size, cache_type)\n",
    "\n",
    "    mem_managers: Dict[str, MemoryManager] = {}\n",
    "    for cache_type, capacity in HARDWARE_CACHE_LIMITS.items():\n",
    "        mem_managers[cache_type] = MemoryManager(capacity, cache_type)\n",
    "    \n",
    "    final_schedule: List[int] = []\n",
    "    memory_allocation_history: Dict[int, int] = {} \n",
    "    spill_log: List[str] = []             \n",
    "    total_extra_data_movement = 0\n",
    "    \n",
    "    new_node_id_counter = max(node_index.keys()) + 1\n",
    "    new_nodes: Dict[int, dict] = {}\n",
    "\n",
    "    for nid in schedule:\n",
    "        current_node = node_index[nid]\n",
    "        op = current_node['Op']\n",
    "        \n",
    "        if op == 'ALLOC':\n",
    "            try:\n",
    "                buf_id = int(current_node['BufId'])\n",
    "                if buf_id not in buf_info_map:\n",
    "                    final_schedule.append(nid)\n",
    "                    continue\n",
    "                    \n",
    "                size, cache_type = buf_info_map[buf_id]\n",
    "                if cache_type not in mem_managers:\n",
    "                    final_schedule.append(nid)\n",
    "                    continue\n",
    "\n",
    "                manager = mem_managers[cache_type]\n",
    "                if size > manager.capacity:\n",
    "                    raise RuntimeError(f\"Buffer {buf_id} requested size {size} exceeds total capacity {manager.capacity} of cache {cache_type}\")\n",
    "\n",
    "                offset = manager.allocate(buf_id, size)\n",
    "                if offset is not None:\n",
    "                    memory_allocation_history[buf_id] = offset\n",
    "                    final_schedule.append(nid)\n",
    "                else:\n",
    "                    spilled_ids: Set[int] = set()\n",
    "                    spill_successful = False\n",
    "                    \n",
    "                    while True:\n",
    "                        total_free = manager.get_total_free_space()\n",
    "                        if total_free < size:\n",
    "                            all_spilled = manager.clear_all_blocks()\n",
    "                            for spilled_id in all_spilled:\n",
    "                                if spilled_id in memory_allocation_history:\n",
    "                                    spilled_size, _ = buf_info_map[spilled_id]\n",
    "                                    spill_out, spill_in = create_spill_nodes(\n",
    "                                        new_node_id_counter, spilled_id, spilled_size, cache_type\n",
    "                                    )\n",
    "                                    new_nodes[spill_out[\"Id\"]] = spill_out\n",
    "                                    new_nodes[spill_in[\"Id\"]] = spill_in\n",
    "                                    new_node_id_counter += 2\n",
    "                                    \n",
    "                                    total_extra_data_movement += spilled_size\n",
    "                                    final_schedule.append(spill_out[\"Id\"])\n",
    "                                    spill_log.append(f\"{spilled_id}:{memory_allocation_history[spilled_id]}\")\n",
    "                                    spilled_ids.add(spilled_id)\n",
    "                            \n",
    "                            total_free = manager.get_total_free_space()\n",
    "                            if total_free < size:\n",
    "                                break\n",
    "                        \n",
    "                        victim_buf_id = manager.select_victim_for_spill()\n",
    "                        if victim_buf_id is None:\n",
    "                            break\n",
    "                        \n",
    "                        victim_size, _ = buf_info_map[victim_buf_id]\n",
    "                        spill_out_node, spill_in_node = create_spill_nodes(\n",
    "                            new_node_id_counter, victim_buf_id, victim_size, cache_type\n",
    "                        )\n",
    "                        new_nodes[spill_out_node[\"Id\"]] = spill_out_node\n",
    "                        new_nodes[spill_in_node[\"Id\"]] = spill_in_node\n",
    "                        new_node_id_counter += 2\n",
    "\n",
    "                        total_extra_data_movement += victim_size\n",
    "                        final_schedule.append(spill_out_node[\"Id\"])\n",
    "                        spill_log.append(f\"{victim_buf_id}:{memory_allocation_history.get(victim_buf_id, 'unknown')}\")\n",
    "                        \n",
    "                        manager.free(victim_buf_id)\n",
    "                        spilled_ids.add(victim_buf_id)\n",
    "\n",
    "                        offset = manager.allocate(buf_id, size)\n",
    "                        if offset is not None:\n",
    "                            memory_allocation_history[buf_id] = offset\n",
    "                            final_schedule.append(nid)\n",
    "                            \n",
    "                            for spilled_id in list(spilled_ids):\n",
    "                                spilled_size, _ = buf_info_map[spilled_id]\n",
    "                                new_offset = manager.allocate(spilled_id, spilled_size)\n",
    "                                if new_offset is not None:\n",
    "                                    memory_allocation_history[spilled_id] = new_offset\n",
    "                                    for node in new_nodes.values():\n",
    "                                        if node.get(\"BufId\") == spilled_id and node.get(\"Op\") == \"SPILL_IN\":\n",
    "                                            final_schedule.append(node[\"Id\"])\n",
    "                                            spill_log.append(f\"{spilled_id}:{new_offset}\")\n",
    "                                            break\n",
    "                                    spilled_ids.remove(spilled_id)\n",
    "                            \n",
    "                            spill_successful = True\n",
    "                            break\n",
    "            \n",
    "            except UnboundLocalError:\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error processing ALLOC node {nid}: {str(e)}\") from e\n",
    "\n",
    "        elif op == 'FREE':\n",
    "            try:\n",
    "                buf_id = int(current_node['BufId'])\n",
    "                if buf_id in buf_info_map:\n",
    "                    size, cache_type = buf_info_map[buf_id]\n",
    "                    if cache_type in mem_managers:\n",
    "                        mem_managers[cache_type].free(buf_id)\n",
    "                final_schedule.append(nid)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error processing FREE node {nid}: {str(e)}\") from e\n",
    "                \n",
    "        else:\n",
    "            try:\n",
    "                bufs_used = current_node.get('BufsList', [])\n",
    "                for buf_id in bufs_used:\n",
    "                    if buf_id in buf_info_map:\n",
    "                        size, cache_type = buf_info_map[buf_id]\n",
    "                        if cache_type in mem_managers:\n",
    "                            mem_managers[cache_type].touch(buf_id)\n",
    "                final_schedule.append(nid)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error processing node {nid}: {str(e)}\") from e\n",
    "\n",
    "    current_allocations = {}\n",
    "    for manager in mem_managers.values():\n",
    "        current_allocations.update(manager.get_current_allocations())\n",
    "    combined_allocations = {**memory_allocation_history, **current_allocations}\n",
    "    return final_schedule, combined_allocations, spill_log, total_extra_data_movement\n",
    "\n",
    "# --- Helper Functions (no changes) ---\n",
    "def load_problem1_schedule(task_name: str) -> List[int]:\n",
    "    schedule_path = os.path.join(PROBLEM1_DIR, f\"{task_name}_schedule.txt\")\n",
    "    if not os.path.exists(schedule_path):\n",
    "        raise FileNotFoundError(f\"Problem1 schedule file not found: {schedule_path}\")\n",
    "    with open(schedule_path, 'r', encoding='utf-8') as f:\n",
    "        schedule = [int(line.strip()) for line in f if line.strip()]\n",
    "    return schedule\n",
    "\n",
    "def discover_tasks(data_dir: str) -> List[Tuple[str, str, str]]:\n",
    "    files = os.listdir(data_dir)\n",
    "    nodes = {f[:-10] for f in files if f.endswith(\"_Nodes.csv\")}\n",
    "    edges = {f[:-10] for f in files if f.endswith(\"_Edges.csv\")}\n",
    "    bases = sorted(nodes & edges)\n",
    "    return [(b, f\"{b}_Nodes.csv\", f\"{b}_Edges.csv\") for b in bases]\n",
    "\n",
    "def parse_bufs(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x)\n",
    "    nums, cur = [], ''\n",
    "    for ch in s:\n",
    "        if ch.isdigit():\n",
    "            cur += ch\n",
    "        else:\n",
    "            if cur:\n",
    "                nums.append(int(cur)); cur = ''\n",
    "    if cur:\n",
    "        nums.append(int(cur))\n",
    "    return nums\n",
    "\n",
    "def load_graph(nodes_csv: str, edges_csv: str):\n",
    "    df_nodes = pd.read_csv(nodes_csv)\n",
    "    need_cols = ['Id','Op','BufId','Size','Type','Cycles','Pipe','Bufs']\n",
    "    for c in need_cols:\n",
    "        if c not in df_nodes.columns:\n",
    "            df_nodes[c] = None\n",
    "    df_nodes['Id'] = pd.to_numeric(df_nodes['Id'], errors='coerce').astype(int)\n",
    "    for c in ['BufId','Size','Cycles']:\n",
    "        df_nodes[c] = pd.to_numeric(df_nodes[c], errors='coerce')\n",
    "    df_nodes['BufsList'] = df_nodes['Bufs'].apply(parse_bufs)\n",
    "    \n",
    "    df_edges = pd.read_csv(edges_csv)\n",
    "    cols = {c.lower(): c for c in df_edges.columns}\n",
    "    if 'startnodeid' not in cols or 'endnodeid' not in cols:\n",
    "        raise ValueError(\"Edges CSV must contain columns: StartNodeId, EndNodeId\")\n",
    "    s_col, e_col = cols['startnodeid'], cols['endnodeid']\n",
    "    df_edges[s_col] = pd.to_numeric(df_edges[s_col], errors='coerce').astype(int)\n",
    "    df_edges[e_col] = pd.to_numeric(df_edges[e_col], errors='coerce').astype(int)\n",
    "\n",
    "    node_ids = set(df_nodes['Id'].tolist())\n",
    "    succ = {int(nid): [] for nid in node_ids}\n",
    "    pred = {int(nid): [] for nid in node_ids}\n",
    "    for _, e in df_edges.iterrows():\n",
    "        u, v = int(e[s_col]), int(e[e_col])\n",
    "        if u in node_ids and v in node_ids:\n",
    "            succ[u].append(v)\n",
    "            pred[v].append(u)\n",
    "    return df_nodes, succ, pred\n",
    "\n",
    "# --- Core Modification: CSV Output Format + Step Numbering in Schedule ---\n",
    "def save_problem2_results(task_name: str, final_schedule: List[int], memory_allocation: Dict[int, int], spill_log: List[str], metrics: dict):\n",
    "    \"\"\"\n",
    "    1. Schedule file: CSV with two columns (Step: 1-based sequence number, NodeId: node ID)\n",
    "    2. Memory file: CSV with two columns (BufId: buffer ID, Offset: address offset)\n",
    "    3. Spill file: CSV with two columns (BufId: buffer ID, NewOffset: new address offset)\n",
    "    \"\"\"\n",
    "    # 1. Save schedule with step numbering\n",
    "    sched_df = pd.DataFrame({\n",
    "        'Step': list(range(1, len(final_schedule) + 1)),  # 1-based step index\n",
    "        'NodeId': final_schedule\n",
    "    })\n",
    "    sched_path = os.path.join(OUT_DIR, f\"{task_name}_schedule.csv\")\n",
    "    sched_df.to_csv(sched_path, index=False, encoding='utf-8')  # Do not write DataFrame index\n",
    "\n",
    "    # 2. Save memory allocation results\n",
    "    if memory_allocation:\n",
    "        mem_df = pd.DataFrame({\n",
    "            'BufId': list(memory_allocation.keys()),\n",
    "            'Offset': list(memory_allocation.values())\n",
    "        }).sort_values('BufId')  # Sort by BufId for readability\n",
    "    else:\n",
    "        # Preserve header even when empty\n",
    "        mem_df = pd.DataFrame(columns=['BufId', 'Offset'])\n",
    "    mem_path = os.path.join(OUT_DIR, f\"{task_name}_memory.csv\")\n",
    "    mem_df.to_csv(mem_path, index=False, encoding='utf-8')\n",
    "\n",
    "    # 3. Save SPILL operations (split BufId and NewOffset)\n",
    "    spill_data = []\n",
    "    for entry in spill_log:\n",
    "        if ':' in entry:\n",
    "            buf_id_str, offset_str = entry.split(':', 1)  # Split on first \":\"\n",
    "            if buf_id_str.isdigit() and offset_str.isdigit():\n",
    "                spill_data.append({\n",
    "                    'BufId': int(buf_id_str),\n",
    "                    'NewOffset': int(offset_str)\n",
    "                })\n",
    "    if spill_data:\n",
    "        spill_df = pd.DataFrame(spill_data)\n",
    "    else:\n",
    "        spill_df = pd.DataFrame(columns=['BufId', 'NewOffset'])\n",
    "    spill_path = os.path.join(OUT_DIR, f\"{task_name}_spill.csv\")\n",
    "    spill_df.to_csv(spill_path, index=False, encoding='utf-8')\n",
    "\n",
    "    # 4. Save metrics (keep JSON format for analysis)\n",
    "    metrics_path = os.path.join(OUT_DIR, f\"{task_name}_metrics.json\")\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Console output\n",
    "    print(f\"[File Saved] Task {task_name} output completed:\")\n",
    "    print(f\"  - Schedule: {len(final_schedule)} records → {sched_path}\")\n",
    "    print(f\"  - Memory allocations: {len(memory_allocation)} records → {mem_path}\")\n",
    "    print(f\"  - SPILL operations: {len(spill_data)} records → {spill_path}\")\n",
    "\n",
    "# --- Task Execution Function (adapted for CSV output) ---\n",
    "def run_problem2_for_task(task_name: str, nodes_csv: str, edges_csv: str):\n",
    "    print(f\"\\n[Problem2] Starting task: {task_name}\")\n",
    "    \n",
    "    nodes_path = os.path.join(DATA_DIR, nodes_csv)\n",
    "    edges_path = os.path.join(DATA_DIR, edges_csv)\n",
    "    df_nodes, _, _ = load_graph(nodes_path, edges_path)\n",
    "\n",
    "    schedule_p1 = load_problem1_schedule(task_name)\n",
    "\n",
    "    final_schedule, memory_allocation, spill_log, extra_data = process_schedule_with_memory_management(df_nodes, schedule_p1)\n",
    "\n",
    "    new_makespan = \"Not Computed\"\n",
    "    metrics = {\n",
    "        \"task\": task_name,\n",
    "        \"original_nodes_count\": len(df_nodes),\n",
    "        \"final_nodes_count\": len(final_schedule),\n",
    "        \"spill_operations_count\": len(spill_log) // 2,\n",
    "        \"memory_allocations_count\": len(memory_allocation),\n",
    "        \"total_extra_data_movement\": extra_data,\n",
    "        \"new_makespan_cycles\": new_makespan\n",
    "    }\n",
    "    \n",
    "    save_problem2_results(task_name, final_schedule, memory_allocation, spill_log, metrics)\n",
    "    \n",
    "    print(f\"[Problem2] Completed task: {task_name} | SPILL count: {metrics['spill_operations_count']} | \"\n",
    "          f\"Memory records: {metrics['memory_allocations_count']} | Extra data moved: {metrics['total_extra_data_movement']}\")\n",
    "    return metrics\n",
    "\n",
    "# --- Main Entry Point (no changes) ---\n",
    "if __name__ == \"__main__\":\n",
    "    candidates = discover_tasks(DATA_DIR)\n",
    "    all_metrics = []\n",
    "    for task_name, nodes_fn, edges_fn in candidates:\n",
    "        try:\n",
    "            metrics = run_problem2_for_task(task_name, nodes_fn, edges_fn)\n",
    "            all_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing task {task_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    summary_path = os.path.join(OUT_DIR, \"summary_metrics.json\")\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_metrics, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\n[Problem2] All tasks processed. Summary saved to summary_metrics.json\")\n",
    "    for m in all_metrics:\n",
    "        print(f\"- {m['task']}: SPILL={m['spill_operations_count']}, MemoryRecords={m['memory_allocations_count']}, ExtraData={m['total_extra_data_movement']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from PyQt5 import QtWidgets, QtCore\n",
    "    from PyQt5.QtWidgets import QApplication, QMessageBox, QTableWidgetItem, QDialog, QFileDialog\n",
    "    QT_LIB = \"PyQt5\"\n",
    "    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "except Exception:\n",
    "    from PyQt6 import QtWidgets, QtCore\n",
    "    from PyQt6.QtWidgets import QApplication, QMessageBox, QTableWidgetItem, QDialog, QFileDialog\n",
    "    QT_LIB = \"PyQt6\"\n",
    "    from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"QtAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set Chinese font support for matplotlib\n",
    "plt.rcParams['font.sans-serif'] = [\n",
    "    'SimHei', 'Microsoft YaHei', 'HanaMinA', 'Source Han Sans CN',\n",
    "    'Noto Sans CJK SC', 'WenQuanYi Zen Hei', 'PingFang SC', 'sans-serif'\n",
    "]\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "APP_TITLE = f\"Problem 2: Cache Allocation and SPILL Scheduler ({QT_LIB})\"\n",
    "\n",
    "# Cache capacity configuration (unit: bytes)\n",
    "CAPACITY = {\"L1\":4096, \"UB\":1024, \"L0A\":256, \"L0B\":256, \"L0C\":512}\n",
    "\n",
    "def parse_nodes_edges(csv_dir: Path, task_name: str):\n",
    "    \"\"\"\n",
    "    Parse nodes and edges from CSV files for specific task\n",
    "    Args:\n",
    "        csv_dir: Directory containing CSV files\n",
    "        task_name: Name of the target task\n",
    "    Returns:\n",
    "        Parsed nodes, edges and buffer-related metadata\n",
    "    Raises:\n",
    "        FileNotFoundError: If required CSV files are missing\n",
    "    \"\"\"\n",
    "    nodes_path = csv_dir / f\"{task_name}_Nodes.csv\"\n",
    "    edges_path = csv_dir / f\"{task_name}_Edges.csv\"\n",
    "    if not nodes_path.exists() or not edges_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing CSV files: {task_name}\")\n",
    "\n",
    "    nodes = {}\n",
    "    alloc_of_buf = {}  # buffer id -> allocate node id\n",
    "    free_of_buf  = {}  # buffer id -> free node id\n",
    "    bufs_used_by_node = defaultdict(list)  # node id -> list of buffer ids used\n",
    "    buf_attr = {}  # buffer id -> (type, size)\n",
    "    copyin_uses_buf = set()  # buffers used by COPY_IN operations\n",
    "\n",
    "    with open(nodes_path, newline='', encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            nid = int(row[\"Id\"])\n",
    "            op = (row[\"Op\"] or \"\").strip().upper()\n",
    "            row[\"Op\"] = op\n",
    "            nodes[nid] = row\n",
    "\n",
    "            if op == \"ALLOC\":\n",
    "                buf = int(row[\"BufId\"])\n",
    "                size = int(row[\"Size\"])\n",
    "                typ_raw = (row[\"Type\"] or \"\").strip().upper()\n",
    "                # Correct typo in buffer type naming\n",
    "                typ = (typ_raw\n",
    "                       .replace(\"LOA\",\"L0A\")\n",
    "                       .replace(\"LOB\",\"L0B\")\n",
    "                       .replace(\"LOC\",\"L0C\"))\n",
    "                alloc_of_buf[buf] = nid\n",
    "                buf_attr[buf] = (typ, size)\n",
    "\n",
    "            elif op == \"FREE\":\n",
    "                buf = int(row[\"BufId\"])\n",
    "                free_of_buf[buf] = nid\n",
    "\n",
    "            else:\n",
    "                bufs_str = (row.get(\"Bufs\") or \"\").strip()\n",
    "                if bufs_str:\n",
    "                    # Clean up buffer string format\n",
    "                    s = re.sub(r'[\\\"\\[\\]\\s]', '', bufs_str)\n",
    "                    if s:\n",
    "                        for x in s.split(\",\"):\n",
    "                            if x != \"\":\n",
    "                                b = int(x)\n",
    "                                bufs_used_by_node[nid].append(b)\n",
    "                                if op == \"COPY_IN\":\n",
    "                                    copyin_uses_buf.add(b)\n",
    "\n",
    "    edges = []\n",
    "    with open(edges_path, newline='', encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            edges.append((int(row[\"StartNodeId\"]), int(row[\"EndNodeId\"])))\n",
    "\n",
    "    # Calculate maximum node id + 1 as node count reference\n",
    "    N = max(nodes.keys())+1 if nodes else 0\n",
    "    return nodes, edges, alloc_of_buf, free_of_buf, bufs_used_by_node, buf_attr, copyin_uses_buf, N\n",
    "\n",
    "def read_schedule(problem1_dir: Path, task_name: str):\n",
    "    \"\"\"\n",
    "    Read schedule sequence from Problem1 output file\n",
    "    Args:\n",
    "        problem1_dir: Directory containing Problem1 schedule files\n",
    "        task_name: Name of the target task\n",
    "    Returns:\n",
    "        List of node ids in execution order\n",
    "    Raises:\n",
    "        FileNotFoundError: If schedule file is missing\n",
    "    \"\"\"\n",
    "    path = problem1_dir / f\"{task_name}_schedule.txt\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Problem1 schedule sequence: {path}\")\n",
    "    S = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if s:\n",
    "                S.append(int(s))\n",
    "    return S\n",
    "\n",
    "class FreeList:\n",
    "    \"\"\"\n",
    "    Memory free list management with best-fit allocation strategy\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity:int):\n",
    "        \"\"\"Initialize free list with a single interval covering full capacity\"\"\"\n",
    "        self.intervals = [(0, capacity)]\n",
    "\n",
    "    def _merge(self):\n",
    "        \"\"\"Merge contiguous free intervals to reduce fragmentation\"\"\"\n",
    "        iv = self.intervals\n",
    "        iv.sort()\n",
    "        merged = []\n",
    "        for s,l in iv:\n",
    "            if not merged:\n",
    "                merged.append((s,l))\n",
    "            else:\n",
    "                ps,pl = merged[-1]\n",
    "                # Merge if current interval starts right after previous ends\n",
    "                if ps+pl == s:\n",
    "                    merged[-1] = (ps,pl+l)\n",
    "                else:\n",
    "                    merged.append((s,l))\n",
    "        self.intervals = merged\n",
    "\n",
    "    def alloc_best_fit(self, size:int):\n",
    "        \"\"\"\n",
    "        Allocate memory using best-fit strategy\n",
    "        Args:\n",
    "            size: Required memory size\n",
    "        Returns:\n",
    "            Start offset if allocation succeeds, None otherwise\n",
    "        \"\"\"\n",
    "        best_idx, best_len = -1, None\n",
    "        for i,(s,l) in enumerate(self.intervals):\n",
    "            if l>=size:\n",
    "                if best_len is None or l<best_len:\n",
    "                    best_idx, best_len = i, l\n",
    "        if best_idx<0:\n",
    "            return None\n",
    "        s,l = self.intervals[best_idx]\n",
    "        start = s\n",
    "        # Remove interval if exact match, otherwise shrink it\n",
    "        if l == size:\n",
    "            self.intervals.pop(best_idx)\n",
    "        else:\n",
    "            self.intervals[best_idx] = (s+size, l-size)\n",
    "        return start\n",
    "\n",
    "    def free(self, start:int, size:int):\n",
    "        \"\"\"\n",
    "        Free memory interval and merge contiguous intervals\n",
    "        Args:\n",
    "            start: Start offset of memory to free\n",
    "            size: Size of memory to free\n",
    "        \"\"\"\n",
    "        self.intervals.append((start,size))\n",
    "        self._merge()\n",
    "\n",
    "class AllocState:\n",
    "    \"\"\"Data structure to track buffer allocation state\"\"\"\n",
    "    __slots__ = (\"buf\",\"off\",\"size\",\"next_use_pos\")\n",
    "    def __init__(self, buf, off, size, next_use_pos):\n",
    "        self.buf = buf          # buffer id\n",
    "        self.off = off          # memory offset\n",
    "        self.size = size        # buffer size\n",
    "        self.next_use_pos = next_use_pos  # next usage position in schedule\n",
    "\n",
    "def solve_task(csv_dir:Path, problem1_dir:Path, problem2_dir:Path, task_name:str):\n",
    "    \"\"\"\n",
    "    Main logic to solve Problem 2: cache allocation with SPILL scheduling\n",
    "    Args:\n",
    "        csv_dir: Directory with CSV input files\n",
    "        problem1_dir: Directory with Problem1 schedule files\n",
    "        problem2_dir: Directory for Problem2 output files\n",
    "        task_name: Name of target task\n",
    "    Returns:\n",
    "        Dictionary containing task execution metrics\n",
    "    \"\"\"\n",
    "    (nodes, edges, alloc_of_buf, free_of_buf, bufs_used_by_node,\n",
    "     buf_attr, copyin_uses_buf, N0) = parse_nodes_edges(csv_dir, task_name)\n",
    "    S = read_schedule(problem1_dir, task_name)\n",
    "    # Map node id to its position in schedule\n",
    "    pos = {nid:i for i,nid in enumerate(S)}\n",
    "\n",
    "    # Track all usage positions of each buffer\n",
    "    uses = defaultdict(list)\n",
    "    for nid in S:\n",
    "        for b in bufs_used_by_node.get(nid,[]):\n",
    "            uses[b].append(pos[nid])\n",
    "\n",
    "    # Initialize free lists for different cache types\n",
    "    freelists = {t:FreeList(CAPACITY[t]) for t in CAPACITY.keys()}\n",
    "    # Track live (allocated) buffers per cache type\n",
    "    live = {t:{} for t in CAPACITY.keys()}\n",
    "    extra_movement = 0  # Total extra data movement caused by SPILL operations\n",
    "    spill_ops = []      # Record of SPILL operations (buffer id, offset)\n",
    "    next_new_id = N0    # Next available id for new SPILL nodes\n",
    "\n",
    "    # Track first allocation offset of each buffer\n",
    "    first_offset = {}\n",
    "\n",
    "    # New nodes to insert before specific positions in schedule\n",
    "    inserts_before = defaultdict(list)\n",
    "\n",
    "    # Cursor to track next usage position for each buffer\n",
    "    use_cursor = {b:0 for b in buf_attr.keys()}\n",
    "    def next_use_pos_of(b, curpos):\n",
    "        \"\"\"\n",
    "        Find next usage position of buffer after current position\n",
    "        Args:\n",
    "            b: Buffer id\n",
    "            curpos: Current position in schedule\n",
    "        Returns:\n",
    "            Next usage position (infinite if no future use)\n",
    "        \"\"\"\n",
    "        arr = uses.get(b,[])\n",
    "        k = use_cursor[b]\n",
    "        # Move cursor past positions <= current position\n",
    "        while k < len(arr) and arr[k] <= curpos:\n",
    "            k += 1\n",
    "        use_cursor[b] = k\n",
    "        return arr[k] if k < len(arr) else float(\"inf\")\n",
    "\n",
    "    def evict_one(type_name, curpos):\n",
    "        \"\"\"\n",
    "        Evict one buffer from specified cache type based on eviction strategy\n",
    "        Args:\n",
    "            type_name: Cache type (L1/UB/L0A/L0B/L0C)\n",
    "            curpos: Current position in schedule\n",
    "        Returns:\n",
    "            (evicted buffer id, buffer size)\n",
    "        \"\"\"\n",
    "        nonlocal extra_movement, next_new_id\n",
    "        pool = list(live[type_name].values())\n",
    "        if not pool:\n",
    "            return (None, 0)\n",
    "        \n",
    "        if type_name in (\"L0A\", \"L0B\", \"L0C\"):\n",
    "            # Eviction strategy for L0A/B/C: \n",
    "            # Prioritize evicting buffers with no future use or farthest next use\n",
    "            nofuture = [x for x in pool if x.next_use_pos == float(\"inf\")]\n",
    "            if nofuture:\n",
    "                victim = nofuture[0]  # Any buffer with no future use\n",
    "            else:\n",
    "                victim = max(pool, key=lambda x: x.next_use_pos)  # Farthest next use\n",
    "        else:\n",
    "            # Eviction strategy for L1/UB: cost-weighted Belady-like\n",
    "            # Priority P(b*) = (distance to next use) * (spill_cost / size)\n",
    "            priorities = []\n",
    "            for item in pool:\n",
    "                # Calculate spill cost per unit size\n",
    "                cost_per_unit = 0\n",
    "                spill_out_c = 0 if item.buf in copyin_uses_buf else item.size\n",
    "                spill_in_c = item.size\n",
    "                cost_per_unit = (spill_out_c + spill_in_c) / item.size if item.size > 0 else 0\n",
    "\n",
    "                # Time distance to next use\n",
    "                time_dist = (item.next_use_pos - curpos) if item.next_use_pos != float(\"inf\") else float(\"inf\")\n",
    "                \n",
    "                # Adjustment: Buffers with infinite next_use_pos should be evicted first\n",
    "                # Multiply by -1 to make max() select the buffer most needing eviction\n",
    "                priority_score = -time_dist * cost_per_unit\n",
    "                if time_dist == float(\"inf\"):\n",
    "                    priority_score = float(\"-inf\")  # Lowest score (highest eviction priority)\n",
    "\n",
    "                priorities.append((priority_score, item))\n",
    "            \n",
    "            victim = max(priorities, key=lambda x: x[0])[1]\n",
    "\n",
    "        b = victim.buf\n",
    "        spill_out_id = next_new_id\n",
    "        next_new_id += 1\n",
    "        inserts_before[curpos].append(spill_out_id)\n",
    "        \n",
    "        # Count extra data movement for spill operations\n",
    "        if b not in copyin_uses_buf:\n",
    "            extra_movement += victim.size  # SpillOut cost\n",
    "            extra_movement += victim.size  # SpillIn cost\n",
    "        \n",
    "        # Free the evicted buffer's memory\n",
    "        freelists[type_name].free(victim.off, victim.size)\n",
    "        del live[type_name][b]\n",
    "        return (b, victim.size)\n",
    "\n",
    "    def ensure_alloc(b, curpos):\n",
    "        \"\"\"\n",
    "        Ensure buffer is allocated (evict other buffers if necessary)\n",
    "        Args:\n",
    "            b: Buffer id to allocate\n",
    "            curpos: Current position in schedule\n",
    "        Returns:\n",
    "            Allocated memory offset\n",
    "        Raises:\n",
    "            RuntimeError: If allocation fails even after eviction\n",
    "        \"\"\"\n",
    "        nonlocal extra_movement, next_new_id\n",
    "        typ, size = buf_attr[b]\n",
    "\n",
    "        # Special handling for L0A/B/C: evict all live buffers first\n",
    "        if typ in (\"L0A\",\"L0B\",\"L0C\"):\n",
    "            while live[typ]:\n",
    "                evict_one(typ, curpos)\n",
    "\n",
    "        # Try allocation with eviction loop\n",
    "        while True:\n",
    "            off = freelists[typ].alloc_best_fit(size)\n",
    "            if off is not None:\n",
    "                nu = next_use_pos_of(b, curpos)\n",
    "                live[typ][b] = AllocState(b, off, size, nu)\n",
    "                if b not in first_offset:\n",
    "                    first_offset[b] = off\n",
    "                return off\n",
    "            \n",
    "            # Evict one buffer and retry if allocation fails\n",
    "            victim_buf, _ = evict_one(typ, curpos)\n",
    "            if victim_buf is None:\n",
    "                live_bufs = list(live[typ].keys())\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to allocate {typ} at position {curpos} (Buffer ID: {b}, Size: {size}, Capacity: {CAPACITY[typ]}). \"\n",
    "                    f\"Currently resident Buffer IDs: {live_bufs}. No buffers available for eviction.\"\n",
    "                )\n",
    "\n",
    "    # Preprocess buffers used by each node\n",
    "    bufs_of_node = defaultdict(list)\n",
    "    for nid in nodes:\n",
    "        for b in bufs_used_by_node.get(nid,[]):\n",
    "            bufs_of_node[nid].append(b)\n",
    "\n",
    "    new_nodes_added = 0\n",
    "    # Process each node in schedule sequence\n",
    "    for i, nid in enumerate(S):\n",
    "        row = nodes[nid]\n",
    "        op = (row[\"Op\"] or \"\").strip().upper()\n",
    "\n",
    "        # Handle buffers used by current node\n",
    "        for b in bufs_of_node.get(nid, []):\n",
    "            typ_size = buf_attr.get(b)\n",
    "            if not typ_size:\n",
    "                continue\n",
    "            typ, size = typ_size\n",
    "            # Allocate buffer if not already live (add SPILL_IN node)\n",
    "            if b not in live[typ]:\n",
    "                spill_in_id = next_new_id\n",
    "                next_new_id+=1\n",
    "                inserts_before[i].append(spill_in_id)\n",
    "                new_nodes_added += 1\n",
    "                \n",
    "                extra_movement += size  # Count SPILL_IN data movement\n",
    "                \n",
    "                off = ensure_alloc(b, i)\n",
    "                spill_ops.append((b, off))\n",
    "\n",
    "        # Handle ALLOC/FREE operations\n",
    "        if op == \"ALLOC\":\n",
    "            b = int(row[\"BufId\"])\n",
    "            ensure_alloc(b, i)\n",
    "        elif op == \"FREE\":\n",
    "            b = int(row[\"BufId\"])\n",
    "            typ, size = buf_attr[b]\n",
    "            if b in live[typ]:\n",
    "                # Free buffer memory\n",
    "                freelists[typ].free(live[typ][b].off, size)\n",
    "                del live[typ][b]\n",
    "        else:\n",
    "            # Update next use position for live buffers\n",
    "            for b in bufs_of_node.get(nid, []):\n",
    "                typ, size = buf_attr[b]\n",
    "                if b in live[typ]:\n",
    "                    live[typ][b].next_use_pos = next_use_pos_of(b, i)\n",
    "\n",
    "    # Generate new schedule with inserted SPILL nodes\n",
    "    new_schedule = []\n",
    "    for i, nid in enumerate(S):\n",
    "        if i in inserts_before:\n",
    "            new_schedule.extend(inserts_before[i])\n",
    "        new_schedule.append(nid)\n",
    "\n",
    "    # Create output directory and write results\n",
    "    problem2_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Write new schedule\n",
    "    with open(problem2_dir/f\"{task_name}_schedule.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for x in new_schedule:\n",
    "            f.write(str(x)+\"\\n\")\n",
    "    # Write buffer first allocation offsets\n",
    "    with open(problem2_dir/f\"{task_name}_memory.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for b in sorted(first_offset.keys()):\n",
    "            f.write(f\"{b}:{first_offset[b]}\\n\")\n",
    "    # Write SPILL operations\n",
    "    with open(problem2_dir/f\"{task_name}_spill.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for b,off in spill_ops:\n",
    "            f.write(f\"{b}:{off}\\n\")\n",
    "\n",
    "    # Return task metrics\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"extra_movement\": int(extra_movement),\n",
    "        \"spill_count\": len(spill_ops),\n",
    "        \"new_nodes_added\": new_nodes_added,\n",
    "        \"schedule_len\": len(new_schedule)\n",
    "    }\n",
    "\n",
    "class MplCanvas(FigureCanvas):\n",
    "    \"\"\"Matplotlib canvas wrapper for Qt integration\"\"\"\n",
    "    def __init__(self, parent=None, width=6, height=3.6, dpi=110):\n",
    "        fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        super().__init__(fig)\n",
    "        self.setParent(parent)\n",
    "        self.ax = ax\n",
    "        self.fig = fig\n",
    "\n",
    "class ChartDialog(QDialog):\n",
    "    \"\"\"Dialog window for displaying matplotlib charts\"\"\"\n",
    "    def __init__(self, parent, title, plot_fn):\n",
    "        super().__init__(parent)\n",
    "        self.setWindowTitle(title)\n",
    "        self.resize(820, 520)\n",
    "        v = QtWidgets.QVBoxLayout(self)\n",
    "        self.canvas = MplCanvas(self, width=6.8, height=3.8, dpi=110)\n",
    "        v.addWidget(self.canvas)\n",
    "        # Execute plot function to draw chart\n",
    "        plot_fn(self.canvas.ax)\n",
    "        self.canvas.fig.tight_layout()\n",
    "        self.canvas.draw()\n",
    "        # Add close button\n",
    "        btn = QtWidgets.QDialogButtonBox(QtWidgets.QDialogButtonBox.StandardButton.Close if QT_LIB==\"PyQt6\"\n",
    "                                         else QtWidgets.QDialogButtonBox.Close)\n",
    "        btn.rejected.connect(self.reject)\n",
    "        btn.accepted.connect(self.accept)\n",
    "        v.addWidget(btn)\n",
    "\n",
    "class MainWin(QtWidgets.QMainWindow):\n",
    "    \"\"\"Main application window\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(APP_TITLE)\n",
    "        self.resize(1000, 680)\n",
    "        w = QtWidgets.QWidget()\n",
    "        self.setCentralWidget(w)\n",
    "        main_layout = QtWidgets.QVBoxLayout(w)\n",
    "\n",
    "        # Button row\n",
    "        btn_row = QtWidgets.QHBoxLayout()\n",
    "        self.btn_select_csv = QtWidgets.QPushButton(\"Select CSV Version Directory\")\n",
    "        self.btn_select_prob1 = QtWidgets.QPushButton(\"Select Problem1 Directory\")\n",
    "        self.btn_run = QtWidgets.QPushButton(\"Run Problem 2 (All Test Cases)\")\n",
    "        self.btn_copy = QtWidgets.QPushButton(\"Copy Table to Clipboard\")\n",
    "        self.btn_chart1 = QtWidgets.QPushButton(\"View Chart 1 (Extra Data Movement per Task)\")\n",
    "        self.btn_chart2 = QtWidgets.QPushButton(\"View Chart 2 (SPILL Count vs New Schedule Length)\")\n",
    "        btn_row.addWidget(self.btn_select_csv)\n",
    "        btn_row.addWidget(self.btn_select_prob1)\n",
    "        btn_row.addWidget(self.btn_run)\n",
    "        btn_row.addStretch(1)\n",
    "        btn_row.addWidget(self.btn_copy)\n",
    "        btn_row.addWidget(self.btn_chart1)\n",
    "        btn_row.addWidget(self.btn_chart2)\n",
    "        main_layout.addLayout(btn_row)\n",
    "\n",
    "        # Path display label\n",
    "        self.lbl_paths = QtWidgets.QLabel(\"Please select CSV Version and Problem1 directories.\")\n",
    "        self.lbl_paths.setWordWrap(True)\n",
    "        main_layout.addWidget(self.lbl_paths)\n",
    "\n",
    "        # Results table\n",
    "        self.table = QtWidgets.QTableWidget(0,5)\n",
    "        self.table.setHorizontalHeaderLabels([\"Task\",\"Extra Data Movement\",\"SPILL Count\",\"New Nodes Added\",\"New Schedule Length\"])\n",
    "        self.table.horizontalHeader().setStretchLastSection(True)\n",
    "        main_layout.addWidget(self.table, stretch=1)\n",
    "\n",
    "        # Status bar\n",
    "        self.status = QtWidgets.QLabel(\"Ready.\")\n",
    "        main_layout.addWidget(self.status)\n",
    "\n",
    "        # Connect button signals\n",
    "        self.btn_select_csv.clicked.connect(self.on_select_csv_dir)\n",
    "        self.btn_select_prob1.clicked.connect(self.on_select_prob1_dir)\n",
    "        self.btn_run.clicked.connect(self.on_run_all)\n",
    "        self.btn_copy.clicked.connect(self.on_copy)\n",
    "        self.btn_chart1.clicked.connect(self.on_show_chart1)\n",
    "        self.btn_chart2.clicked.connect(self.on_show_chart2)\n",
    "\n",
    "        # Initialize directory variables\n",
    "        self.csv_dir = None\n",
    "        self.prob1_dir = None\n",
    "        self.prob2_dir = None\n",
    "        # List of test tasks\n",
    "        self.tasks = [\n",
    "            \"Conv_Case0\",\"Conv_Case1\",\n",
    "            \"FlashAttention_Case0\",\"FlashAttention_Case1\",\n",
    "            \"Matmul_Case0\",\"Matmul_Case1\",\n",
    "        ]\n",
    "        self.results_cache = []  # Cache for task results\n",
    "\n",
    "    def update_path_display(self):\n",
    "        \"\"\"Update path display label with current directory selections\"\"\"\n",
    "        csv_path_str = str(self.csv_dir) if self.csv_dir else \"Not selected\"\n",
    "        prob1_path_str = str(self.prob1_dir) if self.prob1_dir else \"Not selected\"\n",
    "        prob2_path_str = str(self.prob2_dir) if self.prob2_dir else \"To be determined\"\n",
    "        self.lbl_paths.setText(\n",
    "            f\"CSV Version Directory: {csv_path_str}\\nProblem1 Directory: {prob1_path_str}\\nProblem2 (Output) Directory: {prob2_path_str}\"\n",
    "        )\n",
    "        if self.csv_dir and self.prob1_dir:\n",
    "            self.status.setText(\"Paths Ready ✅\")\n",
    "        else:\n",
    "            self.status.setText(\"Please select all required directories.\")\n",
    "\n",
    "    def on_select_csv_dir(self):\n",
    "        \"\"\"Handle CSV directory selection\"\"\"\n",
    "        dir_path = QFileDialog.getExistingDirectory(self, \"Select CSV Version Directory\")\n",
    "        if dir_path:\n",
    "            self.csv_dir = Path(dir_path)\n",
    "            # Set Problem2 output directory based on CSV directory location\n",
    "            self.prob2_dir = self.csv_dir.parent / \"Problem2\" if self.csv_dir.name == \"CSV版本\" else self.csv_dir / \"Problem2\"\n",
    "            self.update_path_display()\n",
    "\n",
    "    def on_select_prob1_dir(self):\n",
    "        \"\"\"Handle Problem1 directory selection\"\"\"\n",
    "        dir_path = QFileDialog.getExistingDirectory(self, \"Select Problem1 Directory\")\n",
    "        if dir_path:\n",
    "            self.prob1_dir = Path(dir_path)\n",
    "            self.update_path_display()\n",
    "\n",
    "    def on_run_all(self):\n",
    "        \"\"\"Run Problem 2 for all test cases\"\"\"\n",
    "        if not (self.csv_dir and self.prob1_dir):\n",
    "            QMessageBox.warning(self, \"Prompt\", \"Please select CSV Version and Problem1 directories first.\")\n",
    "            return\n",
    "        # Clear previous results\n",
    "        self.table.setRowCount(0)\n",
    "        self.results_cache = []\n",
    "        # Process each task\n",
    "        for t in self.tasks:\n",
    "            try:\n",
    "                info = solve_task(self.csv_dir, self.prob1_dir, self.prob2_dir, t)\n",
    "                self.results_cache.append(info)\n",
    "                self.append_row(info)\n",
    "                # Update UI during processing\n",
    "                QApplication.processEvents()\n",
    "            except Exception as e:\n",
    "                QMessageBox.critical(self, \"Error\", f\"{t} failed:\\n{e}\")\n",
    "                return\n",
    "        # Calculate total metrics\n",
    "        total_mv = sum(x[\"extra_movement\"] for x in self.results_cache)\n",
    "        total_sp = sum(x[\"spill_count\"] for x in self.results_cache)\n",
    "        self.status.setText(f\"Completed ✅  Total Extra Data Movement={total_mv}, Total SPILL Count={total_sp}. Results exported to {self.prob2_dir}.\")\n",
    "        QMessageBox.information(self, \"Completed\", \"All test cases for Problem 2 have been processed. Detailed results are in the Problem2 folder.\")\n",
    "\n",
    "    def append_row(self, info):\n",
    "        \"\"\"Append task result row to table\"\"\"\n",
    "        r = self.table.rowCount()\n",
    "        self.table.insertRow(r)\n",
    "        self.table.setItem(r,0,QTableWidgetItem(info[\"task\"]))\n",
    "        self.table.setItem(r,1,QTableWidgetItem(str(info[\"extra_movement\"])))\n",
    "        self.table.setItem(r,2,QTableWidgetItem(str(info[\"spill_count\"])))\n",
    "        self.table.setItem(r,3,QTableWidgetItem(str(info[\"new_nodes_added\"])))\n",
    "        self.table.setItem(r,4,QTableWidgetItem(str(info[\"schedule_len\"])))\n",
    "\n",
    "    def on_copy(self):\n",
    "        \"\"\"Copy table contents to clipboard\"\"\"\n",
    "        rows = self.table.rowCount()\n",
    "        cols = self.table.columnCount()\n",
    "        lines = []\n",
    "        # Get header row\n",
    "        header = [self.table.horizontalHeaderItem(c).text() for c in range(cols)]\n",
    "        lines.append(\"\\t\".join(header))\n",
    "        # Get data rows\n",
    "        for r in range(rows):\n",
    "            row = []\n",
    "            for c in range(cols):\n",
    "                it = self.table.item(r,c)\n",
    "                row.append(it.text() if it else \"\")\n",
    "            lines.append(\"\\t\".join(row))\n",
    "        # Join and copy to clipboard\n",
    "        txt = \"\\n\".join(lines)\n",
    "        cb = QApplication.clipboard()\n",
    "        cb.setText(txt)\n",
    "        QMessageBox.information(self, \"Copied\", \"Statistics table copied to clipboard ✅\")\n",
    "\n",
    "    def on_show_chart1(self):\n",
    "        \"\"\"Show Chart 1: Extra Data Movement per Task (Bar Chart)\"\"\"\n",
    "        if not self.results_cache:\n",
    "            QMessageBox.information(self, \"Prompt\", \"Please run Problem 2 first before viewing charts.\")\n",
    "            return\n",
    "        def plot_bar(ax):\n",
    "            tasks = [x[\"task\"] for x in self.results_cache]\n",
    "            mv    = [x[\"extra_movement\"] for x in self.results_cache]\n",
    "            ax.bar(tasks, mv)\n",
    "            ax.set_title(\"Extra Data Movement by Task\")\n",
    "            ax.set_xlabel(\"Task\")\n",
    "            ax.set_ylabel(\"Extra Data Movement (Bytes)\")\n",
    "            ax.tick_params(axis='x', rotation=30)\n",
    "        dlg = ChartDialog(self, \"Chart 1: Extra Data Movement per Task\", plot_bar)\n",
    "        dlg.exec()\n",
    "\n",
    "    def on_show_chart2(self):\n",
    "        \"\"\"Show Chart 2: SPILL Count vs New Schedule Length (Scatter Plot)\"\"\"\n",
    "        if not self.results_cache:\n",
    "            QMessageBox.information(self, \"Prompt\", \"Please run Problem 2 first before viewing charts.\")\n",
    "            return\n",
    "        def plot_scatter(ax):\n",
    "            tasks = [x[\"task\"] for x in self.results_cache]\n",
    "            sp    = [x[\"spill_count\"] for x in self.results_cache]\n",
    "            slen  = [x[\"schedule_len\"] for x in self.results_cache]\n",
    "            ax.scatter(slen, sp)\n",
    "            # Add task labels to scatter points\n",
    "            for i, name in enumerate(tasks):\n",
    "                ax.annotate(name, (slen[i], sp[i]), xytext=(5,5), textcoords=\"offset points\", fontsize=8)\n",
    "            ax.set_title(\"SPILL Count vs New Schedule Length\")\n",
    "            ax.set_xlabel(\"New Schedule Length\")\n",
    "            ax.set_ylabel(\"SPILL Count\")\n",
    "        dlg = ChartDialog(self, \"Chart 2: SPILL Count vs New Schedule Length\", plot_scatter)\n",
    "        dlg.exec()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Application entry point\"\"\"\n",
    "    app = QApplication(sys.argv)\n",
    "    win = MainWin()\n",
    "    win.show()\n",
    "    try:\n",
    "        sys.exit(app.exec_())\n",
    "    except AttributeError:\n",
    "        # Compatibility for PyQt5/PyQt6 exec() difference\n",
    "        sys.exit(app.exec())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74b317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
