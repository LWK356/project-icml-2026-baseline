{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bca4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调度完成，评估表已输出到: ./Problem1\\Problem1_eval_summary.csv\n",
      "                   task  nodes  ok_perm  ok_topo  ok_nonneg  peak_user  \\\n",
      "0            Conv_Case0   2580     True     True       True      62778   \n",
      "1            Conv_Case1  36086     True     True       True     476790   \n",
      "2  FlashAttention_Case0   1716     True     True       True      42248   \n",
      "3  FlashAttention_Case1   6952     True     True       True     171664   \n",
      "4          Matmul_Case0   4160     True     True       True     131328   \n",
      "5          Matmul_Case1  30976     True     True       True    1048832   \n",
      "\n",
      "   peak_base  improve_vs_base note  \n",
      "0     150807         0.583720       \n",
      "1     570168         0.163773       \n",
      "2      64264         0.342587       \n",
      "3     248976         0.310520       \n",
      "4     163840         0.198437       \n",
      "5    1179648         0.110894       \n"
     ]
    }
   ],
   "source": [
    "# ==== Problem 1: One-Click Execution ====\n",
    "import os, json, csv, glob, heapq\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# ---------- Data Structure ----------\n",
    "class Node:\n",
    "    __slots__ = (\"Id\",\"Op\",\"BufId\",\"Size\",\"Type\",\"Pipe\",\"Cycles\",\"Bufs\")\n",
    "    def __init__(self, Id:int, Op:str,\n",
    "                 BufId:Optional[int]=None, Size:Optional[int]=None, Type:Optional[str]=None,\n",
    "                 Pipe:Optional[str]=None, Cycles:Optional[int]=None, Bufs:Optional[List[int]]=None):\n",
    "        self.Id = Id\n",
    "        self.Op = Op or \"\"\n",
    "        self.BufId = BufId if BufId in (None,\"\") else int(BufId)\n",
    "        self.Size  = 0 if Size  in (None,\"\") else int(Size)\n",
    "        self.Type  = Type\n",
    "        self.Pipe  = Pipe\n",
    "        self.Cycles= 0 if Cycles in (None,\"\") else int(Cycles)\n",
    "        self.Bufs  = [] if Bufs is None else list(Bufs)\n",
    "\n",
    "# ---------- Parser ----------\n",
    "def parse_csv(nodes_csv:str, edges_csv:str) -> Tuple[str, Dict[int,Node], Dict[int,List[int]], Dict[int,List[int]]]:\n",
    "    \"\"\"\n",
    "    Parse CSV files to generate nodes and edges.\n",
    "\n",
    "    Returns:\n",
    "        Task name, node dictionary, adjacency list, reverse adjacency list\n",
    "    \"\"\"\n",
    "    nodes: Dict[int,Node] = {}\n",
    "    with open(nodes_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            Id = int(row[\"Id\"])\n",
    "            nodes[Id] = Node(\n",
    "                Id=Id, Op=row.get(\"Op\",\"\"),\n",
    "                BufId=(None if row.get(\"BufId\") in (None,\"\") else int(row[\"BufId\"])),\n",
    "                Size=(0 if row.get(\"Size\") in (None,\"\") else int(row[\"Size\"])),\n",
    "                Type=(row.get(\"Type\") or None),\n",
    "                Pipe=(row.get(\"Pipe\") or None),\n",
    "                Cycles=(0 if row.get(\"Cycles\") in (None,\"\") else int(row[\"Cycles\"])),\n",
    "                Bufs=[]\n",
    "            )\n",
    "    adj, radj = {i:[] for i in nodes}, {i:[] for i in nodes}\n",
    "    with open(edges_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            u, v = int(row[\"StartNodeId\"]), int(row[\"EndNodeId\"])\n",
    "            adj[u].append(v); radj[v].append(u)\n",
    "    task_name = os.path.basename(nodes_csv).replace(\"_Nodes.csv\",\"\")\n",
    "    return task_name, nodes, adj, radj\n",
    "\n",
    "# ---------- Scheduler ----------\n",
    "class PeakLiveScheduler:\n",
    "    def __init__(self, nodes, adj, radj):\n",
    "        self.nodes, self.adj, self.radj = nodes, adj, radj\n",
    "        self.N = len(nodes)\n",
    "        self.indeg = {i: len(radj[i]) for i in nodes}\n",
    "\n",
    "    def _soon_free_bytes_if_do(self, u, rem_in):\n",
    "        \"\"\"Calculate bytes to be freed if node u is executed.\"\"\"\n",
    "        s = 0\n",
    "        for v in self.adj[u]:\n",
    "            nd = self.nodes[v]\n",
    "            if nd.Op.upper()==\"FREE\" and rem_in[v]==1:\n",
    "                s += nd.Size\n",
    "        return s\n",
    "\n",
    "    def _score(self, u, rem_in):\n",
    "        \"\"\"Priority scoring for scheduling.\"\"\"\n",
    "        nd = self.nodes[u]; op = nd.Op.upper()\n",
    "        if op==\"FREE\":\n",
    "            return (0, -nd.Size, u)\n",
    "        elif op==\"ALLOC\":\n",
    "            return (2, nd.Size, u)\n",
    "        else:\n",
    "            return (1, -self._soon_free_bytes_if_do(u, rem_in), u)\n",
    "\n",
    "    def schedule(self):\n",
    "        \"\"\"Topological sort with peak memory optimization.\"\"\"\n",
    "        rem_in = dict(self.indeg)\n",
    "        heap, inheap = [], set()\n",
    "        for u,deg in rem_in.items():\n",
    "            if deg==0:\n",
    "                heapq.heappush(heap, (self._score(u, rem_in), u)); inheap.add(u)\n",
    "        live, peak, order = 0, 0, []\n",
    "        while heap:\n",
    "            _, u = heapq.heappop(heap)\n",
    "            if rem_in[u]!=0: continue\n",
    "            inheap.discard(u)\n",
    "            order.append(u)\n",
    "            op = self.nodes[u].Op.upper()\n",
    "            if op==\"ALLOC\":\n",
    "                live += self.nodes[u].Size; peak = max(peak, live)\n",
    "            elif op==\"FREE\":\n",
    "                live -= self.nodes[u].Size; live = max(live,0)\n",
    "            for v in self.adj[u]:\n",
    "                rem_in[v]-=1\n",
    "                if rem_in[v]==0 and v not in inheap:\n",
    "                    heapq.heappush(heap,(self._score(v,rem_in),v)); inheap.add(v)\n",
    "        return order, peak\n",
    "\n",
    "# ---------- Output ----------\n",
    "def write_schedule(task_name, schedule, out_dir):\n",
    "    \"\"\"Write scheduling results to file.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, f\"{task_name}_schedule.txt\")\n",
    "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for nid in schedule: f.write(f\"{nid}\\n\")\n",
    "    return out_path\n",
    "\n",
    "def run_task_csv(nodes_csv, edges_csv, out_dir):\n",
    "    \"\"\"Process a single task (nodes + edges CSV pair).\"\"\"\n",
    "    task_name, nodes, adj, radj = parse_csv(nodes_csv, edges_csv)\n",
    "    sch = PeakLiveScheduler(nodes, adj, radj)\n",
    "    order, peak = sch.schedule()\n",
    "    out_file = write_schedule(task_name, order, out_dir)\n",
    "    return {\"task\":task_name,\"nodes\":len(nodes),\"edges\":sum(len(v) for v in adj.values()),\n",
    "            \"peak_live\":peak,\"output\":out_file}\n",
    "\n",
    "def find_and_run_all(base_dir, out_dir):\n",
    "    \"\"\"Batch process all tasks in base_dir.\"\"\"\n",
    "    results=[]\n",
    "    nodes_paths=glob.glob(os.path.join(base_dir,\"*_Nodes.csv\"))\n",
    "    for npth in nodes_paths:\n",
    "        name=os.path.basename(npth).replace(\"_Nodes.csv\",\"\")\n",
    "        epth=os.path.join(base_dir,f\"{name}_Edges.csv\")\n",
    "        if os.path.exists(epth):\n",
    "            try: results.append(run_task_csv(npth,epth,out_dir))\n",
    "            except Exception as e: results.append({\"task\":name,\"error\":str(e)})\n",
    "    return results,out_dir\n",
    "\n",
    "# ---------- Evaluation ----------\n",
    "import pandas as pd\n",
    "def evaluate_all(base_dir, schedule_dir, save_csv_path):\n",
    "    \"\"\"Evaluate all scheduling results against baselines.\"\"\"\n",
    "    def _parse_nodes(nodes_csv):\n",
    "        \"\"\"Parse node CSV file.\"\"\"\n",
    "        nodes={}\n",
    "        with open(nodes_csv,\"r\",encoding=\"utf-8\") as f:\n",
    "            reader=csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                nid=int(row[\"Id\"])\n",
    "                nodes[nid]={\"Op\":row.get(\"Op\",\"\"),\n",
    "                            \"Size\":int(row[\"Size\"]) if row.get(\"Size\") not in (None,\"\") else 0}\n",
    "        return nodes\n",
    "\n",
    "    def _parse_edges(edges_csv):\n",
    "        \"\"\"Parse edge CSV file.\"\"\"\n",
    "        from collections import defaultdict,Counter\n",
    "        adj=defaultdict(list); indeg=Counter()\n",
    "        with open(edges_csv,\"r\",encoding=\"utf-8\") as f:\n",
    "            reader=csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                u,v=int(row[\"StartNodeId\"]),int(row[\"EndNodeId\"])\n",
    "                adj[u].append(v); indeg[v]+=1\n",
    "        return dict(adj),dict(indeg)\n",
    "\n",
    "    def _read_schedule(path):\n",
    "        \"\"\"Read scheduled node sequence from file.\"\"\"\n",
    "        return [int(line.strip()) for line in open(path) if line.strip()]\n",
    "\n",
    "    def _validate_topo(seq,adj):\n",
    "        \"\"\"Check topological validity of the sequence.\"\"\"\n",
    "        pos={nid:i for i,nid in enumerate(seq)}\n",
    "        for u,vs in adj.items():\n",
    "            for v in vs:\n",
    "                if pos[u]>=pos[v]:\n",
    "                    return False,f\"Topological violation {u}->{v}\"\n",
    "        return True,\"\"\n",
    "\n",
    "    def _check_live(seq,nodes):\n",
    "        \"\"\"Verify non-negative memory usage during execution.\"\"\"\n",
    "        live=0\n",
    "        for i,nid in enumerate(seq):\n",
    "            op=nodes[nid][\"Op\"].upper();sz=nodes[nid][\"Size\"]\n",
    "            if op==\"ALLOC\": live+=sz\n",
    "            elif op==\"FREE\": live-=sz\n",
    "            if live<0: return False,f\"Negative memory at {i}\"\n",
    "        return True,\"\"\n",
    "\n",
    "    def _peak(seq,nodes):\n",
    "        \"\"\"Calculate peak memory usage.\"\"\"\n",
    "        live=0;peak=0\n",
    "        for nid in seq:\n",
    "            op=nodes[nid][\"Op\"].upper();sz=nodes[nid][\"Size\"]\n",
    "            if op==\"ALLOC\": live+=sz; peak=max(peak,live)\n",
    "            elif op==\"FREE\": live-=sz\n",
    "        return peak\n",
    "\n",
    "    def _baseline(nodes,adj,indeg):\n",
    "        \"\"\"Baseline topological sort (no memory optimization).\"\"\"\n",
    "        import heapq\n",
    "        rem=indeg.copy()\n",
    "        for nid in nodes: rem.setdefault(nid,0)\n",
    "        ready=[nid for nid,d in rem.items() if d==0]; heapq.heapify(ready)\n",
    "        order=[]\n",
    "        while ready:\n",
    "            u=heapq.heappop(ready); order.append(u)\n",
    "            for v in adj.get(u,[]): rem[v]-=1;\n",
    "            if rem[v]==0: heapq.heappush(ready,v)\n",
    "        return order\n",
    "\n",
    "    rows=[]\n",
    "    for npth in glob.glob(os.path.join(base_dir,\"*_Nodes.csv\")):\n",
    "        name=os.path.basename(npth).replace(\"_Nodes.csv\",\"\")\n",
    "        epth=os.path.join(base_dir,f\"{name}_Edges.csv\")\n",
    "        spth=os.path.join(schedule_dir,f\"{name}_schedule.txt\")\n",
    "        if not (os.path.exists(epth) and os.path.exists(spth)): continue\n",
    "        nodes=_parse_nodes(npth); adj,indeg=_parse_edges(epth); seq=_read_schedule(spth)\n",
    "        ok_perm=(set(seq)==set(nodes.keys())); ok_topo,msg1=_validate_topo(seq,adj)\n",
    "        ok_nonneg,msg2=_check_live(seq,nodes)\n",
    "        peak_user=_peak(seq,nodes); peak_base=_peak(_baseline(nodes,adj,indeg),nodes)\n",
    "        improve=(peak_base-peak_user)/peak_base if peak_base>0 else None\n",
    "        rows.append({\"task\":name,\"nodes\":len(nodes),\n",
    "                     \"ok_perm\":ok_perm,\"ok_topo\":ok_topo,\"ok_nonneg\":ok_nonneg,\n",
    "                     \"peak_user\":peak_user,\"peak_base\":peak_base,\n",
    "                     \"improve_vs_base\":improve,\"note\":msg1 or msg2})\n",
    "    df=pd.DataFrame(rows); os.makedirs(os.path.dirname(save_csv_path),exist_ok=True)\n",
    "    df.to_csv(save_csv_path,index=False); return df\n",
    "\n",
    "# === One-Click Execution ===\n",
    "base_dir = r\"./CSV\"          # Input directory (contains *_Nodes.csv, *_Edges.csv)\n",
    "out_dir  = r\"./Problem1\"     # Output directory (generates *_schedule.txt and evaluation table)\n",
    "\n",
    "# 1) Batch scheduling\n",
    "results, _ = find_and_run_all(base_dir, out_dir)\n",
    "\n",
    "# 2) Evaluation\n",
    "eval_csv = os.path.join(out_dir,\"Problem1_eval_summary.csv\")\n",
    "df_eval = evaluate_all(base_dir, out_dir, eval_csv)\n",
    "\n",
    "print(f\"Scheduling completed. Evaluation results saved to: {eval_csv}\")\n",
    "print(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1189ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据预处理...\n",
      "预处理完成，结果保存到以下CSV文件：\n",
      ".\\CSV版本\\Matmul_Case0_node_attributes.csv\n",
      ".\\CSV版本\\Matmul_Case0_in_degree.csv\n",
      ".\\CSV版本\\Matmul_Case0_buf_lifecycle.csv\n",
      "\n",
      "开始生成最终修复版调度序列...\n",
      "\n",
      "最终调度完成！\n",
      "总节点数：4160 | 已调度节点数：4160 | 未调度节点数：0\n",
      "最大缓存驻留容量: 27904\n",
      "最终调度序列已保存至: .\\CSV版本\\Matmul_Case0_schedule_final.csv\n",
      "\n",
      "调度序列预览（前20个节点）:\n",
      "[0, 5, 6, 14, 15, 23, 24, 32, 33, 41, 42, 50, 51, 59, 60, 68, 69, 74, 77, 78]\n",
      "\n",
      "调度序列预览（后20个节点）:\n",
      "[2933, 2935, 4035, 2934, 2936, 3146, 2937, 4156, 4157, 2938, 2940, 4038, 2939, 2941, 3149, 2942, 4158, 4159, 2943, 4143]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# File directory path\n",
    "file_dir = r'.\\CSV'\n",
    "\n",
    "# --------------------------\n",
    "# Part 1: Data Preprocessing (unchanged)\n",
    "# --------------------------\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Read node and edge files\n",
    "nodes_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_Nodes.csv')\n",
    "edges_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_Edges.csv')\n",
    "\n",
    "# 1. Build node attribute table (retain Cycles field)\n",
    "node_attributes = {}\n",
    "for _, row in nodes_df.iterrows():\n",
    "    node_id = int(row['Id'])\n",
    "    op_type = row['Op']\n",
    "    buf_id = int(row['BufId']) if pd.notna(row['BufId']) else None\n",
    "    size = int(row['Size']) if pd.notna(row['Size']) else None\n",
    "    cache_type = row['Type'] if pd.notna(row['Type']) else None\n",
    "    cycles = int(row['Cycles']) if pd.notna(row['Cycles']) else 0\n",
    "    node_attributes[node_id] = {\n",
    "        'id': node_id,\n",
    "        'op': op_type,\n",
    "        'buf_id': buf_id,\n",
    "        'size': size,\n",
    "        'cache_type': cache_type,\n",
    "        'cycles': cycles,\n",
    "        'predecessors': [],\n",
    "        'successors': []\n",
    "    }\n",
    "\n",
    "# 2. Process edge information (unchanged)\n",
    "in_degree = defaultdict(int)\n",
    "for _, row in edges_df.iterrows():\n",
    "    src = int(row['StartNodeId'])\n",
    "    dst = int(row['EndNodeId'])\n",
    "    node_attributes[dst]['predecessors'].append(src)\n",
    "    node_attributes[src]['successors'].append(dst)\n",
    "    in_degree[dst] += 1\n",
    "    if src not in in_degree:\n",
    "        in_degree[src] = 0\n",
    "\n",
    "# 3. Build buffer lifecycle mapping (unchanged)\n",
    "buf_lifecycle = defaultdict(dict)\n",
    "for node_id, attr in node_attributes.items():\n",
    "    if attr['op'] == 'ALLOC' and attr['buf_id'] is not None:\n",
    "        buf_id = attr['buf_id']\n",
    "        buf_lifecycle[buf_id]['alloc'] = node_id\n",
    "        buf_lifecycle[buf_id]['uses'] = []\n",
    "    elif attr['op'] == 'FREE' and attr['buf_id'] is not None:\n",
    "        buf_id = attr['buf_id']\n",
    "        buf_lifecycle[buf_id]['free'] = node_id\n",
    "\n",
    "# Populate use nodes (unchanged)\n",
    "for buf_id, lifecycle in buf_lifecycle.items():\n",
    "    if 'alloc' not in lifecycle or 'free' not in lifecycle:\n",
    "        continue\n",
    "    alloc_id = lifecycle['alloc']\n",
    "    free_id = lifecycle['free']\n",
    "    visited = set()\n",
    "    queue = [alloc_id]\n",
    "    while queue:\n",
    "        current = queue.pop(0)\n",
    "        if current == free_id:\n",
    "            break\n",
    "        if current in visited:\n",
    "            continue\n",
    "        visited.add(current)\n",
    "        if current != alloc_id:\n",
    "            lifecycle['uses'].append(current)\n",
    "        for succ in node_attributes[current]['successors']:\n",
    "            if succ not in visited:\n",
    "                queue.append(succ)\n",
    "\n",
    "# 4. Save preprocessing results (unchanged)\n",
    "node_attr_df = pd.DataFrame(node_attributes.values())\n",
    "node_attr_df.to_csv(f'{file_dir}\\\\Matmul_Case0_node_attributes.csv', index=False)\n",
    "\n",
    "in_degree_df = pd.DataFrame(list(in_degree.items()), columns=['NodeId', 'InDegree'])\n",
    "in_degree_df.to_csv(f'{file_dir}\\\\Matmul_Case0_in_degree.csv', index=False)\n",
    "\n",
    "buf_lifecycle_list = []\n",
    "for buf_id, data in buf_lifecycle.items():\n",
    "    record = {\n",
    "        'BufId': buf_id,\n",
    "        'AllocNode': data['alloc'],\n",
    "        'UseNodes': ','.join(str(x) for x in data['uses']) if data['uses'] else '',\n",
    "        'FreeNode': data['free']\n",
    "    }\n",
    "    buf_lifecycle_list.append(record)\n",
    "buf_lifecycle_df = pd.DataFrame(buf_lifecycle_list)\n",
    "buf_lifecycle_df.to_csv(f'{file_dir}\\\\Matmul_Case0_buf_lifecycle.csv', index=False)\n",
    "\n",
    "print('Preprocessing completed. Results saved to the following CSV files:')\n",
    "print(f'{file_dir}\\\\Matmul_Case0_node_attributes.csv')\n",
    "print(f'{file_dir}\\\\Matmul_Case0_in_degree.csv')\n",
    "print(f'{file_dir}\\\\Matmul_Case0_buf_lifecycle.csv')\n",
    "\n",
    "# --------------------------\n",
    "# Part 2: Final Fixed Scheduling Sequence Generation\n",
    "# --------------------------\n",
    "print(\"\\nStarting final fixed scheduling sequence generation...\")\n",
    "\n",
    "# 1. Load preprocessing data (unchanged)\n",
    "node_attr_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_node_attributes.csv')\n",
    "node_attributes = {}\n",
    "for _, row in node_attr_df.iterrows():\n",
    "    node_id = int(row['id'])\n",
    "    node_attributes[node_id] = {\n",
    "        'id': node_id,\n",
    "        'op': row['op'],\n",
    "        'buf_id': int(row['buf_id']) if pd.notna(row['buf_id']) else None,\n",
    "        'size': int(row['size']) if pd.notna(row['size']) else None,\n",
    "        'cache_type': row['cache_type'] if pd.notna(row['cache_type']) else None,\n",
    "        'cycles': int(row['cycles']) if pd.notna(row['cycles']) else 0,\n",
    "        'predecessors': eval(row['predecessors']),\n",
    "        'successors': eval(row['successors'])\n",
    "    }\n",
    "\n",
    "in_degree_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_in_degree.csv')\n",
    "in_degree = {int(row['NodeId']): int(row['InDegree']) for _, row in in_degree_df.iterrows()}\n",
    "\n",
    "buf_lifecycle_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_buf_lifecycle.csv')\n",
    "buf_lifecycle = {}\n",
    "for _, row in buf_lifecycle_df.iterrows():\n",
    "    buf_id = int(row['BufId'])\n",
    "    use_nodes_str = str(row['UseNodes']) if pd.notna(row['UseNodes']) else ''\n",
    "    uses = []\n",
    "    if use_nodes_str and use_nodes_str != 'nan' and use_nodes_str != '0':\n",
    "        uses = list(map(int, use_nodes_str.split(',')))\n",
    "    buf_lifecycle[buf_id] = {\n",
    "        'alloc': int(row['AllocNode']),\n",
    "        'uses': uses,\n",
    "        'free': int(row['FreeNode'])\n",
    "    }\n",
    "\n",
    "# 2. Initialize scheduling data structures (new: node status tracking)\n",
    "schedule = []\n",
    "candidate_heap = []\n",
    "current_cache = 0\n",
    "max_cache = 0\n",
    "buf_in_use = set()\n",
    "current_step = 0\n",
    "\n",
    "# New 1: Node status tracking (avoid duplicate scheduling/re-adding)\n",
    "node_status = {node_id: 'pending' for node_id in node_attributes}  # pending/processing/completed\n",
    "retry_count = defaultdict(int)  # Node retry count (limit to 3 times)\n",
    "MAX_RETRY = 3  # Maximum retry attempts, considered invalid beyond\n",
    "\n",
    "# L0 cache progress tracking (unchanged)\n",
    "l0_progress = {\n",
    "    'L0A': {'in_use': False, 'current_op': None, 'progress': 0},\n",
    "    'L0B': {'in_use': False, 'current_op': None, 'progress': 0},\n",
    "    'L0C': {'in_use': False, 'current_op': None, 'progress': 0}\n",
    "}\n",
    "\n",
    "# Future demand queue parameter (unchanged)\n",
    "K = 15\n",
    "\n",
    "# --------------------------\n",
    "# Core Fix 1: Multi-dimensional Priority Calculation (unchanged)\n",
    "# --------------------------\n",
    "def calculate_priority(node, current_step):\n",
    "    priority = 0\n",
    "    buf_id = node['buf_id']\n",
    "    op_type = node['op']\n",
    "\n",
    "    if op_type == 'FREE' and buf_id is not None:\n",
    "        buf_size = node['size'] if (node['size'] is not None and buf_id in buf_lifecycle) else 0\n",
    "        priority += (100 + buf_size) * 0.4\n",
    "\n",
    "    elif op_type in ['ALLOC', 'COPY_IN', 'COPY_OUT', 'MMAD', 'SOFTMAX'] and buf_id in buf_lifecycle:\n",
    "        lifecycle = buf_lifecycle[buf_id]\n",
    "        if 'free' in lifecycle and 'alloc' in lifecycle:\n",
    "            total_lifecycle = lifecycle['free'] - lifecycle['alloc']\n",
    "            if total_lifecycle > 0:\n",
    "                current_reside = current_step - lifecycle['alloc']\n",
    "                lifecycle_ratio = min(current_reside / total_lifecycle, 1.0)\n",
    "                priority += lifecycle_ratio * 100 * 0.3\n",
    "\n",
    "    if op_type not in ['ALLOC', 'FREE']:\n",
    "        priority += node['cycles'] * 0.2\n",
    "\n",
    "    if node['cache_type'] in ['L0A', 'L0B', 'L0C']:\n",
    "        priority += 50 * 0.1\n",
    "\n",
    "    return priority\n",
    "\n",
    "# --------------------------\n",
    "# Core Fix 2: Conflict Prediction Function (unchanged)\n",
    "# --------------------------\n",
    "def predict_cache_conflict(future_alloc_list, cache_type, current_cache_used):\n",
    "    cache_capacity_map = {'L1': 4096, 'UB': 1024, 'L0A': 256, 'L0B': 256, 'L0C': 512}\n",
    "    if cache_type not in cache_capacity_map:\n",
    "        return False\n",
    "    cache_capacity = cache_capacity_map[cache_type]\n",
    "    future_total = sum(alloc['size'] for alloc in future_alloc_list if alloc['cache_type'] == cache_type)\n",
    "    return (current_cache_used + future_total) > cache_capacity\n",
    "\n",
    "# Initialize candidate pool (new: only add pending status nodes)\n",
    "for node_id, degree in in_degree.items():\n",
    "    if degree == 0 and node_status[node_id] == 'pending':\n",
    "        node = node_attributes[node_id]\n",
    "        priority = calculate_priority(node, current_step=0)\n",
    "        heapq.heappush(candidate_heap, (-priority, node_id))\n",
    "        node_status[node_id] = 'processing'  # Mark as processing to avoid duplicate adding\n",
    "\n",
    "# --------------------------\n",
    "# Core Fix 3: Scheduling Main Loop (solve infinite loop)\n",
    "# --------------------------\n",
    "# Record processed node count to check if all nodes are scheduled\n",
    "total_nodes = len(node_attributes)\n",
    "while candidate_heap:\n",
    "    # Termination condition 1: All nodes have been scheduled\n",
    "    if len(schedule) >= total_nodes:\n",
    "        print(f\"Early termination: All {total_nodes} nodes have been scheduled\")\n",
    "        break\n",
    "\n",
    "    current_step = len(schedule)\n",
    "\n",
    "    # Step 1: Update future demand queue (fixed: process only pending/processing nodes)\n",
    "    temp_heap = candidate_heap.copy()\n",
    "    future_alloc_queue = []\n",
    "    count = 0\n",
    "    while temp_heap and count < K:\n",
    "        try:\n",
    "            neg_prio, node_id = heapq.heappop(temp_heap)\n",
    "        except IndexError:\n",
    "            break\n",
    "        if node_status[node_id] != 'completed':  # Process only uncompleted nodes\n",
    "            node = node_attributes[node_id]\n",
    "            if node['op'] == 'ALLOC' and node['cache_type'] is not None and node['buf_id'] is not None:\n",
    "                future_alloc_queue.append({\n",
    "                    'buf_id': node['buf_id'],\n",
    "                    'size': node['size'] if node['size'] is not None else 0,\n",
    "                    'cache_type': node['cache_type']\n",
    "                })\n",
    "        count += 1\n",
    "\n",
    "    # Step 2: Pop current highest priority node (fixed: filter completed nodes)\n",
    "    valid_node = False\n",
    "    while candidate_heap and not valid_node:\n",
    "        try:\n",
    "            neg_prio, current_node_id = heapq.heappop(candidate_heap)\n",
    "        except IndexError:\n",
    "            break\n",
    "        # Filter condition: node not completed and retry count not exceeded\n",
    "        if node_status[current_node_id] == 'completed':\n",
    "            continue\n",
    "        if retry_count[current_node_id] > MAX_RETRY:\n",
    "            print(f\"Node {current_node_id} exceeded max retry count ({MAX_RETRY}), marked as invalid\")\n",
    "            node_status[current_node_id] = 'completed'\n",
    "            continue\n",
    "        # Found valid node, exit filtering loop\n",
    "        current_node = node_attributes[current_node_id]\n",
    "        valid_node = True\n",
    "\n",
    "    # Termination condition 2: No valid nodes to schedule (heap not empty but all invalid)\n",
    "    if not valid_node:\n",
    "        print(f\"Terminated: No valid nodes available in candidate pool (scheduled {len(schedule)}/{total_nodes} nodes)\")\n",
    "        break\n",
    "\n",
    "    # Step 3: L0 cache fine scheduling judgment (fixed: retry count limit)\n",
    "    l0_skip = False\n",
    "    if current_node['op'] == 'ALLOC' and current_node['cache_type'] in ['L0A', 'L0B', 'L0C']:\n",
    "        l0_type = current_node['cache_type']\n",
    "        l0_info = l0_progress[l0_type]\n",
    "        buf_id = current_node['buf_id']\n",
    "        if buf_id is None:\n",
    "            retry_count[current_node_id] += 1\n",
    "            heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "            l0_skip = True\n",
    "            continue\n",
    "\n",
    "        if l0_info['in_use']:\n",
    "            if l0_info['progress'] >= 90:\n",
    "                free_node_id = next((n for n in node_attributes if\n",
    "                                     node_attributes[n]['op'] == 'FREE' and\n",
    "                                     node_attributes[n]['buf_id'] == buf_id and\n",
    "                                     node_status[n] == 'pending'), None)  # Only find pending FREE nodes\n",
    "                if free_node_id and in_degree[free_node_id] == 0:\n",
    "                    free_node = node_attributes[free_node_id]\n",
    "                    free_prio = calculate_priority(free_node, current_step)\n",
    "                    # Execute FREE node scheduling\n",
    "                    if buf_id in buf_in_use:\n",
    "                        current_cache -= free_node['size'] if free_node['size'] is not None else 0\n",
    "                        buf_in_use.remove(buf_id)\n",
    "                    schedule.append(free_node_id)\n",
    "                    node_status[free_node_id] = 'completed'  # Mark as completed\n",
    "                    # Update L0 status\n",
    "                    l0_progress[l0_type]['in_use'] = False\n",
    "                    l0_progress[l0_type]['current_op'] = None\n",
    "                    l0_progress[l0_type]['progress'] = 0\n",
    "                else:\n",
    "                    retry_count[current_node_id] += 1\n",
    "                    heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "                    l0_skip = True\n",
    "            else:\n",
    "                retry_count[current_node_id] += 1\n",
    "                heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "                l0_skip = True\n",
    "        else:\n",
    "            if buf_id in buf_lifecycle:\n",
    "                use_node_id = next((n for n in buf_lifecycle[buf_id].get('uses', [])\n",
    "                                    if node_attributes[n]['op'] == 'MMAD' and\n",
    "                                    node_status[n] == 'pending'), None)  # Only find pending usage nodes\n",
    "                if use_node_id:\n",
    "                    l0_progress[l0_type]['in_use'] = True\n",
    "                    l0_progress[l0_type]['current_op'] = use_node_id\n",
    "    if l0_skip:\n",
    "        continue\n",
    "\n",
    "    # Step 4: Buffer conflict prediction (fixed: retry count limit)\n",
    "    alloc_skip = False\n",
    "    if current_node['op'] == 'ALLOC' and current_node['cache_type'] is not None and current_node['buf_id'] is not None:\n",
    "        cache_type = current_node['cache_type']\n",
    "        buf_id = current_node['buf_id']\n",
    "        buf_size = current_node['size'] if current_node['size'] is not None else 0\n",
    "\n",
    "        # Calculate current cache used capacity (fixed: only count valid buf_id)\n",
    "        current_cache_used = 0\n",
    "        for bid in buf_in_use:\n",
    "            if bid in buf_lifecycle and buf_lifecycle[bid].get('alloc') in node_attributes:\n",
    "                alloc_node = node_attributes[buf_lifecycle[bid]['alloc']]\n",
    "                if alloc_node['cache_type'] == cache_type and alloc_node['size'] is not None:\n",
    "                    current_cache_used += alloc_node['size']\n",
    "\n",
    "        if predict_cache_conflict(future_alloc_queue, cache_type, current_cache_used):\n",
    "            # Filter pending and in-degree 0 FREE nodes\n",
    "            free_node_candidates = [\n",
    "                n for n in node_attributes\n",
    "                if (node_attributes[n]['op'] == 'FREE' and\n",
    "                    node_attributes[n]['cache_type'] == cache_type and\n",
    "                    node_attributes[n]['buf_id'] is not None and\n",
    "                    in_degree[n] == 0 and\n",
    "                    node_status[n] == 'pending')\n",
    "            ]\n",
    "\n",
    "            if free_node_candidates:\n",
    "                # Select FREE node with maximum release capacity\n",
    "                def get_free_size(node_id):\n",
    "                    node = node_attributes[node_id]\n",
    "                    return node['size'] if node['size'] is not None else 0\n",
    "\n",
    "                free_node_id = max(free_node_candidates, key=get_free_size)\n",
    "                free_node = node_attributes[free_node_id]\n",
    "                free_buf_id = free_node['buf_id']\n",
    "\n",
    "                # Release cache\n",
    "                if free_buf_id in buf_in_use:\n",
    "                    current_cache -= get_free_size(free_node_id)\n",
    "                    buf_in_use.remove(free_buf_id)\n",
    "                    if free_node['cache_type'] in ['L0A', 'L0B', 'L0C']:\n",
    "                        l0_progress[free_node['cache_type']]['in_use'] = False\n",
    "                        l0_progress[free_node['cache_type']]['current_op'] = None\n",
    "                        l0_progress[free_node['cache_type']]['progress'] = 0\n",
    "\n",
    "                # Schedule FREE node\n",
    "                schedule.append(free_node_id)\n",
    "                node_status[free_node_id] = 'completed'\n",
    "                # Re-add current ALLOC node (retry count +1)\n",
    "                retry_count[current_node_id] += 1\n",
    "                heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "                alloc_skip = True\n",
    "    if alloc_skip:\n",
    "        continue\n",
    "\n",
    "    # Step 5: Execute current node scheduling (mark as completed)\n",
    "    schedule.append(current_node_id)\n",
    "    node_status[current_node_id] = 'completed'  # Key: Mark as completed after scheduling to avoid duplication\n",
    "    retry_count[current_node_id] = 0  # Reset retry count\n",
    "\n",
    "    # Step 6: Update L0 operation node progress (unchanged)\n",
    "    for l0_type in l0_progress:\n",
    "        l0_info = l0_progress[l0_type]\n",
    "        if l0_info['in_use'] and l0_info['current_op'] == current_node_id:\n",
    "            step_progress = 100 / max(current_node['cycles'], 1)\n",
    "            l0_info['progress'] = min(l0_info['progress'] + step_progress, 100)\n",
    "            buf_id = current_node['buf_id']\n",
    "            if buf_id is not None and buf_id in buf_lifecycle and 'free' in buf_lifecycle[buf_id]:\n",
    "                free_node_id = buf_lifecycle[buf_id]['free']\n",
    "                if free_node_id in node_attributes and in_degree[free_node_id] == 0 and node_status[free_node_id] == 'pending':\n",
    "                    free_node = node_attributes[free_node_id]\n",
    "                    free_prio = calculate_priority(free_node, current_step + 1)\n",
    "                    heapq.heappush(candidate_heap, (-free_prio, free_node_id))\n",
    "                    node_status[free_node_id] = 'processing'\n",
    "\n",
    "    # Step 7: Update cache status (unchanged)\n",
    "    buf_id = current_node['buf_id']\n",
    "    if current_node['op'] == 'ALLOC' and buf_id is not None:\n",
    "        if buf_id not in buf_in_use:\n",
    "            buf_size = current_node['size'] if current_node['size'] is not None else 0\n",
    "            current_cache += buf_size\n",
    "            buf_in_use.add(buf_id)\n",
    "\n",
    "    elif current_node['op'] == 'FREE' and buf_id is not None:\n",
    "        if buf_id in buf_in_use:\n",
    "            buf_size = current_node['size'] if current_node['size'] is not None else 0\n",
    "            current_cache -= buf_size\n",
    "            buf_in_use.remove(buf_id)\n",
    "\n",
    "    # Step 8: Update maximum cache (unchanged)\n",
    "    if current_cache > max_cache:\n",
    "        max_cache = current_cache\n",
    "\n",
    "    # Step 9: Update successor node in-degree (fixed: only add pending nodes)\n",
    "    for succ_id in current_node['successors']:\n",
    "        if succ_id not in node_attributes or node_status[succ_id] == 'completed':\n",
    "            continue  # Skip non-existent or completed nodes\n",
    "        in_degree[succ_id] -= 1\n",
    "        if in_degree[succ_id] == 0 and node_status[succ_id] == 'pending':\n",
    "            succ_node = node_attributes[succ_id]\n",
    "            succ_prio = calculate_priority(succ_node, current_step + 1)\n",
    "            heapq.heappush(candidate_heap, (-succ_prio, succ_id))\n",
    "            node_status[succ_id] = 'processing'  # Mark as processing\n",
    "\n",
    "# 4. Result saving and output (new: verify scheduling completeness)\n",
    "schedule_df = pd.DataFrame({'NodeId': schedule})\n",
    "schedule_df.to_csv(f'{file_dir}\\\\Matmul_Case0_schedule_final.csv', index=False)\n",
    "\n",
    "# Output scheduling completeness information\n",
    "completed_nodes = set(schedule)\n",
    "missing_nodes = [node_id for node_id in node_attributes if node_id not in completed_nodes]\n",
    "print(f\"\\nFinal scheduling completed!\")\n",
    "print(f\"Total nodes: {total_nodes} | Scheduled nodes: {len(schedule)} | Unscheduled nodes: {len(missing_nodes)}\")\n",
    "if missing_nodes:\n",
    "    print(f\"Unscheduled node IDs: {missing_nodes[:10]}...\")  # Print first 10 unscheduled nodes\n",
    "print(f\"Maximum cache resident capacity: {max_cache}\")\n",
    "print(f\"Final scheduling sequence saved to: {file_dir}\\\\Matmul_Case0_schedule_final.csv\")\n",
    "print(\"\\nScheduling sequence preview (first 20 nodes):\")\n",
    "print(schedule[:20] if len(schedule) >= 20 else schedule)\n",
    "print(\"\\nScheduling sequence preview (last 20 nodes):\")\n",
    "print(schedule[-20:] if len(schedule) >= 20 else schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde20fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
