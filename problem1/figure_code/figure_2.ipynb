{
 "cells": [
  {
   "cell_type": "code",
   "id": "45ab7d3c",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Plotting dependency: matplotlib (do not install seaborn)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rcParams\n",
    "# Keep font settings for possible Chinese display (if needed), can be removed if not required\n",
    "rcParams['font.sans-serif'] = ['SimHei']\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ============= Configuration Area =============\n",
    "# Directory where your data is located: /mnt/data in this conversation environment\n",
    "BASE_DIR = Path(\"./A1\")  # For local running, you can change to Path(\".\") or your absolute path\n",
    "# Keep DEFAULT_CASES for automatic scanning; otherwise, list them manually\n",
    "DEFAULT_CASES = [\n",
    "    \"Conv_Case0\", \"Conv_Case1\",\n",
    "    \"Matmul_Case0\", \"Matmul_Case1\",\n",
    "    \"FlashAttention_Case0\", \"FlashAttention_Case1\"\n",
    "]\n",
    "\n",
    "# Directory for output images and summary CSV\n",
    "OUT_DIR = Path(\"./plots\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ============= Utility Functions =============\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def pick(df: pd.DataFrame, names):\n",
    "    \"\"\"Match a column name from multiple possible names;\n",
    "    if exact match fails, perform partial containment match.\"\"\"\n",
    "    names = [n.lower() for n in names]\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return n\n",
    "    for c in df.columns:\n",
    "        for n in names:\n",
    "            if n in c:\n",
    "                return c\n",
    "    raise KeyError(f\"None of {names} found in columns: {df.columns.tolist()}\")\n",
    "\n",
    "def load_case(case_name: str, base: Path):\n",
    "    nodes = normalize_cols(pd.read_csv(base / f\"{case_name}_node_attributes.csv\"))\n",
    "    sched = normalize_cols(pd.read_csv(base / f\"{case_name}_schedule_final.csv\"))\n",
    "    life  = normalize_cols(pd.read_csv(base / f\"{case_name}_buf_lifecycle.csv\"))\n",
    "    return nodes, sched, life\n",
    "\n",
    "def schedule_pos_map(sched: pd.DataFrame):\n",
    "    node_col = pick(sched, [\"nodeid\",\"id\",\"node_id\"])\n",
    "    order = sched[node_col].tolist()\n",
    "    return {int(n): i for i, n in enumerate(order)}\n",
    "\n",
    "def compute_peaks_and_lifetimes(nodes, sched, life):\n",
    "    \"\"\"Return: layered peak dictionary & life2 table containing size/lifetime\"\"\"\n",
    "    id_col    = pick(nodes, [\"id\",\"nodeid\",\"node_id\"])\n",
    "    op_col    = pick(nodes, [\"op\",\"opcode\",\"operation\"])\n",
    "    buf_col   = pick(nodes, [\"bufid\",\"buf_id\",\"buffer\",\"buffer_id\"])\n",
    "    size_col  = pick(nodes, [\"size\",\"bytes\"])\n",
    "    cache_col = pick(nodes, [\"type\",\"cache_type\",\"level\",\"cachelevel\"])\n",
    "\n",
    "    alloc_nodes = nodes[nodes[op_col].str.upper()==\"ALLOC\"].copy()\n",
    "    alloc_nodes[buf_col] = alloc_nodes[buf_col].astype(int)\n",
    "    buf_map = (alloc_nodes\n",
    "               .set_index(buf_col)[[size_col, cache_col, id_col]]\n",
    "               .rename(columns={size_col:\"size\", cache_col:\"cache\", id_col:\"alloc_node\"}))\n",
    "\n",
    "    pos = schedule_pos_map(sched)\n",
    "\n",
    "    life_buf  = pick(life, [\"bufid\",\"buf_id\"])\n",
    "    life_alloc= pick(life, [\"allocnode\",\"alloc_node\"])\n",
    "    life_free = pick(life, [\"freenode\",\"free_node\"])\n",
    "\n",
    "    life2 = life.copy()\n",
    "    life2[life_buf] = life2[life_buf].astype(int)\n",
    "    life2[\"alloc_idx\"] = life2[life_alloc].astype(int).map(pos)\n",
    "    life2[\"free_idx\"]  = life2[life_free].astype(int).map(pos)\n",
    "    life2 = life2.merge(buf_map, left_on=life_buf, right_index=True, how=\"left\")\n",
    "\n",
    "    # Event scanning to calculate layered peaks\n",
    "    events = {}\n",
    "    for _, r in life2.iterrows():\n",
    "        size = int(r[\"size\"]) if pd.notna(r[\"size\"]) else 0\n",
    "        cache = str(r[\"cache\"]) if pd.notna(r[\"cache\"]) else \"NA\"\n",
    "        ai = int(r[\"alloc_idx\"]); fi = int(r[\"free_idx\"])\n",
    "        events.setdefault(cache, []).append((ai, +size))\n",
    "        events.setdefault(cache, []).append((fi + 1e-6, -size))  # epsilon to make deallocation later than synchronous allocation\n",
    "\n",
    "    peaks = {}\n",
    "    for cache, evs in events.items():\n",
    "        evs.sort(key=lambda x: x[0])\n",
    "        cur = 0; peak = 0\n",
    "        for _, delta in evs:\n",
    "            cur += delta\n",
    "            if cur > peak:\n",
    "                peak = cur\n",
    "        peaks[cache] = int(peak)\n",
    "\n",
    "    # Buffer lifetime (number of steps)\n",
    "    life2[\"lifetime_steps\"] = life2[\"free_idx\"] - life2[\"alloc_idx\"] + 1\n",
    "    return peaks, life2\n",
    "\n",
    "def compute_op_counts(nodes):\n",
    "    op_col = pick(nodes, [\"op\",\"opcode\",\"operation\"])\n",
    "    ops = nodes[op_col].str.upper().value_counts().to_dict()\n",
    "    exclude = {\"ALLOC\",\"FREE\",\"COPY_IN\",\"COPY_OUT\",\"MOVE\"}\n",
    "    compute_ops = sum(cnt for k, cnt in ops.items() if k not in exclude)\n",
    "    return {\"COPY_IN\": ops.get(\"COPY_IN\",0),\n",
    "            \"COPY_OUT\": ops.get(\"COPY_OUT\",0),\n",
    "            \"MOVE\": ops.get(\"MOVE\",0),\n",
    "            \"COMPUTE_ops\": compute_ops}\n",
    "\n",
    "def residency_curve_by_level(nodes, sched, life):\n",
    "    \"\"\"Optional: Generate 'residency vs step curve' (total and layered).\n",
    "    Return steps, total, {level: curve}\"\"\"\n",
    "    peaks, life2 = compute_peaks_and_lifetimes(nodes, sched, life)\n",
    "    pos = schedule_pos_map(sched)\n",
    "    T = len(pos)  # Use scheduling steps as the x-axis\n",
    "    levels = [\"L1\",\"L0A\",\"L0B\",\"L0C\"]\n",
    "    curves = {lv: np.zeros(T, dtype=np.int64) for lv in levels}\n",
    "    total  = np.zeros(T, dtype=np.int64)\n",
    "\n",
    "    for _, r in life2.iterrows():\n",
    "        sz = int(r[\"size\"]) if pd.notna(r[\"size\"]) else 0\n",
    "        if sz <= 0: continue\n",
    "        ai = int(r[\"alloc_idx\"]); fi = int(r[\"free_idx\"])\n",
    "        cache = str(r[\"cache\"]) if pd.notna(r[\"cache\"]) else \"NA\"\n",
    "        sl = slice(ai, fi+1)\n",
    "        total[sl] += sz\n",
    "        if cache in curves:\n",
    "            curves[cache][sl] += sz\n",
    "    steps = np.arange(T)\n",
    "    return steps, total, curves\n",
    "\n",
    "# ============= Load Data & Calculate Metrics =============\n",
    "cases = [c for c in DEFAULT_CASES\n",
    "         if (BASE_DIR / f\"{c}_node_attributes.csv\").exists()\n",
    "         and (BASE_DIR / f\"{c}_schedule_final.csv\").exists()\n",
    "         and (BASE_DIR / f\"{c}_buf_lifecycle.csv\").exists()]\n",
    "\n",
    "if not cases:\n",
    "    raise FileNotFoundError(f\"No target case files found in {BASE_DIR}, please check the path or file naming.\")\n",
    "\n",
    "summary_rows = []\n",
    "lifetimes_by_case = {}\n",
    "size_life_scatter = {}\n",
    "\n",
    "for case in sorted(cases):\n",
    "    nodes, sched, life = load_case(case, BASE_DIR)\n",
    "    peaks, life2 = compute_peaks_and_lifetimes(nodes, sched, life)\n",
    "    counts = compute_op_counts(nodes)\n",
    "\n",
    "    peak_L1  = peaks.get(\"L1\",0)\n",
    "    peak_L0A = peaks.get(\"L0A\",0)\n",
    "    peak_L0B = peaks.get(\"L0B\",0)\n",
    "    peak_L0C = peaks.get(\"L0C\",0)\n",
    "    peak_total = peak_L1 + peak_L0A + peak_L0B + peak_L0C\n",
    "\n",
    "    lifetimes_by_case[case] = life2[\"lifetime_steps\"].dropna().astype(int).values\n",
    "    size_life_scatter[case] = life2[[\"size\",\"lifetime_steps\"]].dropna().astype(float).values\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"case\": case,\n",
    "        \"peak_total\": peak_total,\n",
    "        \"peak_L1\": peak_L1,\n",
    "        \"peak_L0A\": peak_L0A,\n",
    "        \"peak_L0B\": peak_L0B,\n",
    "        \"peak_L0C\": peak_L0C,\n",
    "        **counts\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(summary_rows).sort_values(\"case\").reset_index(drop=True)\n",
    "summary.to_csv(OUT_DIR / \"plots_summary_metrics.csv\", index=False)\n",
    "display(summary)\n",
    "\n",
    "# ============= Plotting (Basic Version for Academic Papers) =============\n",
    "# Note: According to platform requirements, use matplotlib for charts;\n",
    "# each chart has a separate canvas; do not specify color styles.\n",
    "\n",
    "# Figure 1: Total Peak Residency for Each Case\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(summary[\"case\"], summary[\"peak_total\"])\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Cache Peak (bytes)\")\n",
    "plt.title(\"Cache Status for Each Task\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_peak_total_by_case.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Figure 2: Stacked Bar Chart of Peaks by Cache Level\n",
    "cases_order = summary[\"case\"].tolist()\n",
    "L1  = summary.set_index(\"case\").loc[cases_order, \"peak_L1\"].values\n",
    "L0A = summary.set_index(\"case\").loc[cases_order, \"peak_L0A\"].values\n",
    "L0B = summary.set_index(\"case\").loc[cases_order, \"peak_L0B\"].values\n",
    "L0C = summary.set_index(\"case\").loc[cases_order, \"peak_L0C\"].values\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "b1 = plt.bar(cases_order, L1, label=\"L1\")\n",
    "b2 = plt.bar(cases_order, L0A, bottom=L1, label=\"L0A\")\n",
    "b3 = plt.bar(cases_order, L0B, bottom=L1+L0A, label=\"L0B\")\n",
    "b4 = plt.bar(cases_order, L0C, bottom=L1+L0A+L0B, label=\"L0C\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Cache Peak (bytes)\")\n",
    "plt.title(\"Peak Values at Different Cache Levels\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_stacked_peaks_by_cache.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Figure 3: Buffer Lifetime Distribution (Box Plot, Logarithmic Y-axis)\n",
    "data = [lifetimes_by_case[c] for c in cases_order]\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.boxplot(data, labels=cases_order, showfliers=False)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Lifetime (steps)\")\n",
    "plt.title(\"Buffer Lifetime Distribution\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_lifetime_boxplot.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Figure 4: Operator Composition (Grouped Bar Chart)\n",
    "cols = [\"COPY_IN\",\"COPY_OUT\",\"MOVE\",\"COMPUTE_ops\"]\n",
    "x = np.arange(len(cases_order)); width = 0.18\n",
    "plt.figure(figsize=(9,5))\n",
    "for i, col in enumerate(cols):\n",
    "    plt.bar(x + i*width - 1.5*width,\n",
    "            summary.set_index(\"case\").loc[cases_order, col].values,\n",
    "            width, label=col)\n",
    "plt.xticks(x, cases_order, rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Grouped Statistics of Various Operator Counts\")\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_op_counts_grouped.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Figure 5: Size-Lifetime Scatter Plot (Double Logarithmic), Matmul_Case1 as representative by default\n",
    "rep = \"Matmul_Case1\" if \"Matmul_Case1\" in size_life_scatter else cases_order[-1]\n",
    "xy = size_life_scatter[rep]\n",
    "if len(xy) > 0:\n",
    "    plt.figure(figsize=(6.5,5.5))\n",
    "    plt.scatter(xy[:,0], xy[:,1], s=8, alpha=0.5)\n",
    "    plt.xscale(\"log\"); plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Size (bytes, log)\")\n",
    "    plt.ylabel(\"Lifetime (steps, log)\")\n",
    "    plt.title(f\"Scatter Plot of Size vs Lifetime: {rep}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / f\"plot_size_vs_lifetime_{rep}.png\", dpi=200)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"[Tip] No valid size-lifetime data for {rep}, skipping scatter plot.\")\n",
    "\n",
    "print(\"\\nGenerated files are saved in: \", OUT_DIR.resolve())\n",
    "print(\"Including: plots_summary_metrics.csv, 5 PNG images.\")\n",
    "\n",
    "# ============= Optional: Residency vs Step Curve (Uncomment the following code block) =============\n",
    "case_to_draw = \"Matmul_Case1\"  # Can be changed to other case names\n",
    "nodes, sched, life = load_case(case_to_draw, BASE_DIR)\n",
    "steps, total, curves = residency_curve_by_level(nodes, sched, life)\n",
    "plt.figure(figsize=(9,4.5))\n",
    "plt.plot(steps, total, label=\"Total\")\n",
    "for lv, arr in curves.items():\n",
    "    if arr.sum() > 0:\n",
    "        plt.plot(steps, arr, label=lv, linewidth=1)\n",
    "plt.xlabel(\"Scheduling Steps\"); plt.ylabel(\"Cache (bytes)\")\n",
    "plt.title(f\"Cache vs Scheduling Steps: {case_to_draw}\")\n",
    "plt.legend(ncol=5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / f\"plot_residency_curve_{case_to_draw}.png\", dpi=200)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca0297e6",
   "metadata": {},
   "source": [
    "# ================= Correlation Coefficient Heatmap =================\n",
    "# Note: Only use matplotlib; no dependency on seaborn;\n",
    "# automatically filter numerical columns and calculate Pearson correlation coefficients\n",
    "\n",
    "def plot_corr_heatmap(df: pd.DataFrame, title: str, outfile: Path):\n",
    "    \"\"\"Calculate the correlation coefficient matrix for numerical columns of df and plot it as a heatmap\"\"\"\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    if num.shape[1] < 2:\n",
    "        print(\"[Tip] Fewer than 2 numerical columns available for correlation analysis, skipping plot generation.\")\n",
    "        return\n",
    "    corr = num.corr(method=\"pearson\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 5.5))\n",
    "    im = ax.imshow(corr, aspect=\"auto\", interpolation=\"nearest\")  # Default color scheme, no manual specification\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Axis ticks and labels\n",
    "    ax.set_xticks(np.arange(corr.shape[1]))\n",
    "    ax.set_xticklabels(corr.columns, rotation=30, ha=\"right\")\n",
    "    ax.set_yticks(np.arange(corr.shape[0]))\n",
    "    ax.set_yticklabels(corr.index)\n",
    "\n",
    "    # Value annotation\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[1]):\n",
    "            ax.text(j, i, f\"{corr.iloc[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Color bar\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "# 1) Correlation of \"Summary Metrics\" across tasks (Source: summary)\n",
    "plot_corr_heatmap(\n",
    "    df=summary,\n",
    "    title=\"Correlation Coefficients of Summary Metrics Across Tasks\",\n",
    "    outfile=OUT_DIR / \"heatmap_corr_summary.png\"\n",
    ")\n",
    "\n",
    "# 2) Correlation of \"Buffer Features\" for a single task (size / lifetime / alloc_idx / free_idx)\n",
    "def per_case_corr_heatmap(case_name: str):\n",
    "    nodes, sched, life = load_case(case_name, BASE_DIR)\n",
    "    _, life2 = compute_peaks_and_lifetimes(nodes, sched, life)\n",
    "    df_case = life2[[\"size\", \"lifetime_steps\", \"alloc_idx\", \"free_idx\"]].dropna()\n",
    "    if len(df_case) < 3:\n",
    "        print(f\"[Tip] Insufficient valid samples for {case_name}, skipping.\")\n",
    "        return\n",
    "    plot_corr_heatmap(\n",
    "        df=df_case,\n",
    "        title=f\"Correlation Coefficients of Buffer Features for {case_name}\",\n",
    "        outfile=OUT_DIR / f\"heatmap_corr_{case_name}.png\"\n",
    "    )\n",
    "\n",
    "# Specify a representative task to output the heatmap (can be changed to the case name you want to view)\n",
    "per_case_corr_heatmap(\"Matmul_Case1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "499235ef",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Descriptive Statistics and Visualization for Problem 1 Data\n",
    "- Automatically discover *_Nodes.csv and *_Edges.csv files\n",
    "- Generate 6 plots: Node vs Edge Count Comparison, ALLOC vs FREE Count Comparison,\n",
    "  ALLOC Size Box Plot, ALLOC Size Empirical Distribution, ALLOC Type Distribution,\n",
    "  Global Op Distribution\n",
    "- Export a summary table: problem1_data_summary.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========= Configurable Items =========\n",
    "base_dir = \"./CSV\"                 # Directory containing 12 CSV files, e.g., \"./CSV\" or \"/mnt/data\"\n",
    "out_dir  = \"./plots_problem1_data\"  # Directory for output images and tables\n",
    "SAVE_PDF = False               # If True, also save PDF vector images simultaneously\n",
    "# =====================================\n",
    "\n",
    "# Attempt to set Chinese fonts to avoid garbled Chinese titles\n",
    "try:\n",
    "    from matplotlib import rcParams\n",
    "    rcParams[\"font.sans-serif\"] = [\"SimHei\", \"Arial Unicode MS\", \"Noto Sans CJK SC\"]\n",
    "    rcParams[\"axes.unicode_minus\"] = False\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Discover files\n",
    "node_files = sorted(glob.glob(os.path.join(base_dir, \"*_Nodes.csv\")))\n",
    "edge_files = sorted(glob.glob(os.path.join(base_dir, \"*_Edges.csv\")))\n",
    "assert len(node_files) == 6 and len(edge_files) == 6, \"6 *_Nodes.csv and 6 *_Edges.csv should be found in base_dir\"\n",
    "\n",
    "def case_key_from_path(p):\n",
    "    name = os.path.basename(p)\n",
    "    return name.replace(\"_Nodes.csv\", \"\").replace(\"_Edges.csv\", \"\")\n",
    "\n",
    "# Load data\n",
    "cases = {}\n",
    "for nf in node_files:\n",
    "    key = case_key_from_path(nf)\n",
    "    nodes = pd.read_csv(nf)\n",
    "    cases.setdefault(key, {})[\"nodes\"] = nodes\n",
    "\n",
    "for ef in edge_files:\n",
    "    key = case_key_from_path(ef)\n",
    "    edges = pd.read_csv(ef)\n",
    "    cases.setdefault(key, {})[\"edges\"] = edges\n",
    "\n",
    "# Summary statistics\n",
    "summary_rows = []\n",
    "alloc_size_by_case = {}\n",
    "type_dist_all = []\n",
    "op_dist_all = []\n",
    "\n",
    "for key in sorted(cases.keys()):\n",
    "    nodes = cases[key].get(\"nodes\", pd.DataFrame())\n",
    "    edges = cases[key].get(\"edges\", pd.DataFrame())\n",
    "\n",
    "    # Column name case insensitivity compatibility\n",
    "    cols = {c.lower(): c for c in nodes.columns}\n",
    "    op_col   = cols.get(\"op\")\n",
    "    buf_col  = cols.get(\"bufid\")\n",
    "    size_col = cols.get(\"size\")\n",
    "    type_col = cols.get(\"type\")\n",
    "\n",
    "    nodes_local = nodes.copy()\n",
    "    if op_col is not None:\n",
    "        nodes_local[op_col] = nodes_local[op_col].astype(str).str.upper()\n",
    "    else:\n",
    "        nodes_local[\"OP\"] = \"UNKNOWN\"\n",
    "        op_col = \"OP\"\n",
    "\n",
    "    # Collect global distribution\n",
    "    op_dist_all.append(nodes_local[op_col].value_counts())\n",
    "\n",
    "    # Event counting\n",
    "    is_alloc = nodes_local[op_col] == \"ALLOC\"\n",
    "    is_free  = nodes_local[op_col] == \"FREE\"\n",
    "\n",
    "    unique_bufids = 0\n",
    "    if buf_col is not None:\n",
    "        buf_series = pd.concat([\n",
    "            nodes_local.loc[is_alloc, buf_col],\n",
    "            nodes_local.loc[is_free, buf_col]\n",
    "        ], ignore_index=True).dropna()\n",
    "        unique_bufids = buf_series.nunique()\n",
    "\n",
    "    # ALLOC Size statistics\n",
    "    alloc_size_sum = None\n",
    "    alloc_size_min = None\n",
    "    alloc_size_max = None\n",
    "    alloc_size_med = None\n",
    "    alloc_size_p90 = None\n",
    "    alloc_size_mean = None\n",
    "\n",
    "    if size_col is not None:\n",
    "        alloc_sizes = pd.to_numeric(nodes_local.loc[is_alloc, size_col], errors=\"coerce\").dropna()\n",
    "        if not alloc_sizes.empty:\n",
    "            alloc_size_sum  = int(alloc_sizes.sum())\n",
    "            alloc_size_min  = int(alloc_sizes.min())\n",
    "            alloc_size_max  = int(alloc_sizes.max())\n",
    "            alloc_size_med  = float(alloc_sizes.median())\n",
    "            alloc_size_p90  = float(np.percentile(alloc_sizes, 90))\n",
    "            alloc_size_mean = float(alloc_sizes.mean())\n",
    "            alloc_size_by_case[key] = alloc_sizes.values\n",
    "\n",
    "    # Type distribution\n",
    "    if type_col is not None:\n",
    "        type_dist_all.append(nodes_local.loc[is_alloc, type_col].astype(str).value_counts())\n",
    "\n",
    "    # Edge counting\n",
    "    edges_count = len(edges)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"case\": key,\n",
    "        \"nodes\": len(nodes),\n",
    "        \"edges\": edges_count,\n",
    "        \"alloc\": int(is_alloc.sum()),\n",
    "        \"free\": int(is_free.sum()),\n",
    "        \"unique_bufid\": int(unique_bufids),\n",
    "        \"alloc_size_sum\": alloc_size_sum,\n",
    "        \"alloc_size_min\": alloc_size_min,\n",
    "        \"alloc_size_max\": alloc_size_max,\n",
    "        \"alloc_size_median\": None if alloc_size_med is None else round(alloc_size_med, 3),\n",
    "        \"alloc_size_p90\": None if alloc_size_p90 is None else round(alloc_size_p90, 3),\n",
    "        \"alloc_size_mean\": None if alloc_size_mean is None else round(alloc_size_mean, 3),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"case\").reset_index(drop=True)\n",
    "summary_csv = os.path.join(out_dir, \"problem1_data_summary.csv\")\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"Summary table saved to: {summary_csv}\")\n",
    "display(summary_df)\n",
    "\n",
    "# ========== Plot 1: Node vs Edge Count Comparison ==========\n",
    "plt.figure(figsize=(9, 5))\n",
    "x = np.arange(len(summary_df))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, summary_df[\"nodes\"], width, label=\"nodes\")\n",
    "plt.bar(x + width/2, summary_df[\"edges\"], width, label=\"edges\")\n",
    "plt.xticks(x, summary_df[\"case\"], rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Node vs Edge Count Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig1_png = os.path.join(out_dir, \"desc_plot_nodes_edges.png\")\n",
    "plt.savefig(fig1_png, dpi=150)\n",
    "if SAVE_PDF:\n",
    "    plt.savefig(os.path.join(out_dir, \"desc_plot_nodes_edges.pdf\"))\n",
    "plt.show()\n",
    "print(f\"Saved: {fig1_png}\")\n",
    "\n",
    "# ========== Plot 2: ALLOC vs FREE Count Comparison ==========\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.bar(x - width/2, summary_df[\"alloc\"], width, label=\"alloc\")\n",
    "plt.bar(x + width/2, summary_df[\"free\"], width, label=\"free\")\n",
    "plt.xticks(x, summary_df[\"case\"], rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"ALLOC vs FREE Event Count Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig2_png = os.path.join(out_dir, \"desc_plot_alloc_free.png\")\n",
    "plt.savefig(fig2_png, dpi=150)\n",
    "if SAVE_PDF:\n",
    "    plt.savefig(os.path.join(out_dir, \"desc_plot_alloc_free.pdf\"))\n",
    "plt.show()\n",
    "print(f\"Saved: {fig2_png}\")\n",
    "\n",
    "# ========== Plot 3: ALLOC Size Box Plot by Case ==========\n",
    "box_data = []\n",
    "labels = []\n",
    "for key in summary_df[\"case\"]:\n",
    "    arr = alloc_size_by_case.get(key, np.array([]))\n",
    "    if arr.size > 0:\n",
    "        box_data.append(arr)\n",
    "        labels.append(key)\n",
    "\n",
    "if box_data:\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.boxplot(box_data, labels=labels, showmeans=True)\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.ylabel(\"Size\")\n",
    "    plt.title(\"ALLOC Size Distribution Box Plot by Case\")\n",
    "    plt.tight_layout()\n",
    "    fig3_png = os.path.join(out_dir, \"desc_plot_alloc_size_boxplot.png\")\n",
    "    plt.savefig(fig3_png, dpi=150)\n",
    "    if SAVE_PDF:\n",
    "        plt.savefig(os.path.join(out_dir, \"desc_plot_alloc_size_boxplot.pdf\"))\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig3_png}\")\n",
    "else:\n",
    "    print(\"No ALLOC Size data available for box plot\")\n",
    "\n",
    "# ========== Plot 4: ALLOC Size Empirical Cumulative Distribution Function (ECDF) by Case ==========\n",
    "def ecdf(data):\n",
    "    data = np.sort(np.asarray(data))\n",
    "    y = np.arange(1, data.size + 1) / data.size\n",
    "    return data, y\n",
    "\n",
    "if box_data:\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for key in summary_df[\"case\"]:\n",
    "        arr = alloc_size_by_case.get(key, np.array([]))\n",
    "        if arr.size > 0:\n",
    "            x_vals, y_vals = ecdf(arr)\n",
    "            plt.step(x_vals, y_vals, where=\"post\", label=key)\n",
    "    plt.xlabel(\"Size\")\n",
    "    plt.ylabel(\"ECDF\")\n",
    "    plt.title(\"ALLOC Size Empirical Cumulative Distribution Function\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    fig4_png = os.path.join(out_dir, \"desc_plot_alloc_size_ecdf.png\")\n",
    "    plt.savefig(fig4_png, dpi=150)\n",
    "    if SAVE_PDF:\n",
    "        plt.savefig(os.path.join(out_dir, \"desc_plot_alloc_size_ecdf.pdf\"))\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig4_png}\")\n",
    "else:\n",
    "    print(\"No ALLOC Size data available for ECDF plot\")\n",
    "\n",
    "# ========== Plot 5: Global ALLOC Type Distribution ==========\n",
    "if type_dist_all:\n",
    "    type_sum = pd.concat(type_dist_all, axis=1).fillna(0).sum(axis=1).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.bar(type_sum.index.astype(str), type_sum.values)\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(\"Cache Level Type Distribution of ALLOC Events\")\n",
    "    plt.tight_layout()\n",
    "    fig5_png = os.path.join(out_dir, \"desc_plot_type_dist_alloc.png\")\n",
    "    plt.savefig(fig5_png, dpi=150)\n",
    "    if SAVE_PDF:\n",
    "        plt.savefig(os.path.join(out_dir, \"desc_plot_type_dist_alloc.pdf\"))\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig5_png}\")\n",
    "else:\n",
    "    print(\"Type column not found or no ALLOC events available\")\n",
    "\n",
    "# ========== Plot 6: Global Node Op Type Distribution ==========\n",
    "if op_dist_all:\n",
    "    op_sum = pd.concat(op_dist_all, axis=1).fillna(0).sum(axis=1).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.bar(op_sum.index.astype(str), op_sum.values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(\"Global Distribution of Node Op Types\")\n",
    "    plt.tight_layout()\n",
    "    fig6_png = os.path.join(out_dir, \"desc_plot_op_dist.png\")\n",
    "    plt.savefig(fig6_png, dpi=150)\n",
    "    if SAVE_PDF:\n",
    "        plt.savefig(os.path.join(out_dir, \"desc_plot_op_dist.pdf\"))\n",
    "    plt.show()\n",
    "    print(f\"Saved: {fig6_png}\")\n",
    "else:\n",
    "    print(\"Op column not found\")\n",
    "\n",
    "print(\"Completed. Output directory:\", os.path.abspath(out_dir))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f36cdcdd",
   "metadata": {},
   "source": [
    "    import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] = ['SimHei']\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Paths\n",
    "old_path = Path(\"./Problem1/Problem1_eval_summary.csv\")\n",
    "new_path = Path(\"./A1/所有案例调度结果汇总.csv\")  # Keep original filename (file path is external to code logic)\n",
    "\n",
    "# Load data\n",
    "old = pd.read_csv(old_path)\n",
    "new = pd.read_csv(new_path)\n",
    "\n",
    "# Standardize case columns\n",
    "old[\"case\"] = old[\"task\"].astype(str).str.strip()\n",
    "new[\"case\"] = new[\"案例名\"].astype(str).str.strip()  # Keep original column name (from input CSV)\n",
    "\n",
    "# Select needed columns\n",
    "old_sub = old[[\"case\",\"peak_user\"]].rename(columns={\"peak_user\":\"peak_old\"})\n",
    "new_sub = new[[\"case\",\"最大缓存驻留容量\"]].rename(columns={\"最大缓存驻留容量\":\"peak_new\"})  # Keep original column name (from input CSV)\n",
    "\n",
    "# Merge datasets for comparison\n",
    "cmp = pd.merge(old_sub, new_sub, on=\"case\", how=\"inner\")\n",
    "cmp[\"reduction_bytes\"] = (cmp[\"peak_old\"] - cmp[\"peak_new\"]).astype(int)\n",
    "cmp[\"reduction_pct\"] = np.where(cmp[\"peak_old\"]>0, (cmp[\"reduction_bytes\"]/cmp[\"peak_old\"])*100.0, np.nan)\n",
    "\n",
    "# Order by reduction percentage in descending order\n",
    "cmp = cmp.sort_values(\"reduction_pct\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save comparison table\n",
    "out_csv = \"./old_vs_new_peak_comparison.csv\"\n",
    "cmp.to_csv(out_csv, index=False)\n",
    "\n",
    "# Plot 1: Side-by-side comparison of old and new peaks\n",
    "plt.figure(figsize=(9,5))\n",
    "x = np.arange(len(cmp))\n",
    "w = 0.35\n",
    "plt.bar(x - w/2, cmp[\"peak_old\"].values, width=w, label=\"Old peak\")\n",
    "plt.bar(x + w/2, cmp[\"peak_new\"].values, width=w, label=\"New peak\")\n",
    "plt.xticks(x, cmp[\"case\"].tolist(), rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Cache Peak (bytes)\")\n",
    "plt.title(\"\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "p1 = \"./plot_peak_old_vs_new.png\"\n",
    "plt.savefig(p1, dpi=220)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Peak reduction percentage\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(cmp[\"case\"].tolist(), cmp[\"reduction_pct\"].values)\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Optimization Ratio (%)\")\n",
    "plt.title(\"\")\n",
    "plt.tight_layout()\n",
    "p2 = \"./plot_peak_reduction_pct.png\"\n",
    "plt.savefig(p2, dpi=220)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Peak reduction in bytes\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(cmp[\"case\"].tolist(), cmp[\"reduction_bytes\"].values)\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Reduction (bytes)\")\n",
    "plt.title(\"New vs Old: Peak reduction (bytes)\")\n",
    "plt.tight_layout()\n",
    "p3 = \"./plot_peak_reduction_bytes.png\"\n",
    "plt.savefig(p3, dpi=220)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8603f49",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
