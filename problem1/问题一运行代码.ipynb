{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\mywork\\liurunhao\\NPU\\问题一\\问题一\n",
      "candidates: [('Conv_Case0', 'Conv_Case0_Nodes.csv', 'Conv_Case0_Edges.csv'), ('Conv_Case1', 'Conv_Case1_Nodes.csv', 'Conv_Case1_Edges.csv'), ('FlashAttention_Case0', 'FlashAttention_Case0_Nodes.csv', 'FlashAttention_Case0_Edges.csv'), ('FlashAttention_Case1', 'FlashAttention_Case1_Nodes.csv', 'FlashAttention_Case1_Edges.csv'), ('Matmul_Case0', 'Matmul_Case0_Nodes.csv', 'Matmul_Case0_Edges.csv'), ('Matmul_Case1', 'Matmul_Case1_Nodes.csv', 'Matmul_Case1_Edges.csv')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 353\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task_name, nodes_file, edges_file \u001b[38;5;129;01min\u001b[39;00m candidates:\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         result = \u001b[43mrun_innovative_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    355\u001b[39m             results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 309\u001b[39m, in \u001b[36mrun_innovative_scheduler\u001b[39m\u001b[34m(task_name, nodes_fn, edges_fn)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    308\u001b[39m df_nodes, succ, pred = load_graph(nodes_path, edges_path)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m scheduler = \u001b[43mInnovativeMemoryScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msucc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] 开始创新优化...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    312\u001b[39m schedule, peak_memory = scheduler.innovative_optimization()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mInnovativeMemoryScheduler.__init__\u001b[39m\u001b[34m(self, df_nodes, succ, pred)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.succ = succ\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.pred = pred\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28mself\u001b[39m.node_ops = \u001b[43m{\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mId\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_nodes\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.node_sizes = {\u001b[38;5;28mint\u001b[39m(r[\u001b[33m'\u001b[39m\u001b[33mId\u001b[39m\u001b[33m'\u001b[39m]): \u001b[38;5;28mint\u001b[39m(r[\u001b[33m'\u001b[39m\u001b[33mSize\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd.isna(r[\u001b[33m'\u001b[39m\u001b[33mSize\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m df_nodes.iterrows()}\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.node_delta = {\u001b[38;5;28mint\u001b[39m(r[\u001b[33m'\u001b[39m\u001b[33mId\u001b[39m\u001b[33m'\u001b[39m]): memory_delta(r) \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m df_nodes.iterrows()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\www22\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\pandas\\core\\frame.py:1586\u001b[39m, in \u001b[36mDataFrame.iterrows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1584\u001b[39m klass = \u001b[38;5;28mself\u001b[39m._constructor_sliced\n\u001b[32m   1585\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, \u001b[38;5;28mself\u001b[39m.values, strict=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m     s = \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1587\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block:\n\u001b[32m   1588\u001b[39m         s._mgr.add_references(\u001b[38;5;28mself\u001b[39m._mgr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\www22\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\pandas\\core\\series.py:514\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy)\u001b[39m\n\u001b[32m    512\u001b[39m         data = data.copy(deep=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     data = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     data = SingleBlockManager.from_array(data, index, refs=refs)\n\u001b[32m    517\u001b[39m NDFrame.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\www22\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\pandas\\core\\construction.py:681\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    670\u001b[39m             subarr = cast(np.ndarray, subarr)\n\u001b[32m    671\u001b[39m             subarr = lib.maybe_convert_objects(\n\u001b[32m    672\u001b[39m                 subarr,\n\u001b[32m    673\u001b[39m                 \u001b[38;5;66;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    678\u001b[39m                 dtype_if_all_nat=np.dtype(\u001b[33m\"\u001b[39m\u001b[33mM8[s]\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    679\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m subarr = \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np.ndarray):\n\u001b[32m    684\u001b[39m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[32m    685\u001b[39m     dtype = cast(np.dtype, dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\www22\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\pandas\\core\\construction.py:729\u001b[39m, in \u001b[36m_sanitize_ndim\u001b[39m\u001b[34m(result, data, dtype, index, allow_2d)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sanitize_ndim\u001b[39m(\n\u001b[32m    719\u001b[39m     result: ArrayLike,\n\u001b[32m    720\u001b[39m     data,\n\u001b[32m   (...)\u001b[39m\u001b[32m    724\u001b[39m     allow_2d: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    725\u001b[39m ) -> ArrayLike:\n\u001b[32m    726\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[33;03m    Ensure we have a 1-dimensional result array.\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mndim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m == \u001b[32m0\u001b[39m:\n\u001b[32m    730\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mresult should be arraylike with ndim > 0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    733\u001b[39m         \u001b[38;5;66;03m# the result that we want\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from typing import List, Tuple  # New import to resolve undefined List\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "DATA_DIR = r\"./CSV版本\"\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"Attachment\", \"Problem1\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "\n",
    "def parse_bufs(x):\n",
    "    \"\"\"Parse Bufs field into a list of integers.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x)\n",
    "    nums, cur = [], ''\n",
    "    for ch in s:\n",
    "        if ch.isdigit():\n",
    "            cur += ch\n",
    "        else:\n",
    "            if cur:\n",
    "                nums.append(int(cur))\n",
    "                cur = ''\n",
    "    if cur:\n",
    "        nums.append(int(cur))\n",
    "    return nums\n",
    "\n",
    "def load_graph(nodes_csv: str, edges_csv: str):\n",
    "    \"\"\"Load graph from CSV files.\"\"\"\n",
    "    df_nodes = pd.read_csv(nodes_csv)\n",
    "    \n",
    "    need_cols = ['Id','Op','BufId','Size','Type','Cycles','Pipe','Bufs']\n",
    "    for c in need_cols:\n",
    "        if c not in df_nodes.columns:\n",
    "            df_nodes[c] = None\n",
    "    \n",
    "    df_nodes['Id'] = pd.to_numeric(df_nodes['Id'], errors='coerce').astype(int)\n",
    "    for c in ['BufId','Size','Cycles']:\n",
    "        df_nodes[c] = pd.to_numeric(df_nodes[c], errors='coerce')\n",
    "    df_nodes['BufsList'] = df_nodes['Bufs'].apply(parse_bufs)\n",
    "\n",
    "    df_edges = pd.read_csv(edges_csv)\n",
    "    cols = {c.lower(): c for c in df_edges.columns}\n",
    "    if 'startnodeid' not in cols or 'endnodeid' not in cols:\n",
    "        raise ValueError(\"Edges CSV must contain columns: StartNodeId, EndNodeId\")\n",
    "    s_col, e_col = cols['startnodeid'], cols['endnodeid']\n",
    "    df_edges[s_col] = pd.to_numeric(df_edges[s_col], errors='coerce').astype(int)\n",
    "    df_edges[e_col] = pd.to_numeric(df_edges[e_col], errors='coerce').astype(int)\n",
    "\n",
    "    node_ids = set(df_nodes['Id'].tolist())\n",
    "    succ = defaultdict(list)\n",
    "    pred = defaultdict(list)\n",
    "    for _, e in df_edges.iterrows():\n",
    "        u, v = int(e[s_col]), int(e[e_col])\n",
    "        if u in node_ids and v in node_ids:\n",
    "            succ[u].append(v)\n",
    "            pred[v].append(u)\n",
    "    \n",
    "    return df_nodes, succ, pred\n",
    "\n",
    "def memory_delta(row) -> int:\n",
    "    \"\"\"Compute memory change caused by a node.\"\"\"\n",
    "    op = str(row['Op'])\n",
    "    if op == 'ALLOC':\n",
    "        return int(row['Size']) if not pd.isna(row['Size']) else 0\n",
    "    if op == 'FREE':\n",
    "        return -int(row['Size']) if not pd.isna(row['Size']) else 0\n",
    "    return 0\n",
    "\n",
    "# ---------- Innovative Algorithm: Two-Stage Hybrid Optimization Strategy ----------\n",
    "\n",
    "class InnovativeMemoryScheduler:\n",
    "    \"\"\"Innovative two-stage hybrid memory scheduling algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, df_nodes, succ, pred):\n",
    "        self.df_nodes = df_nodes\n",
    "        self.succ = succ\n",
    "        self.pred = pred\n",
    "        \n",
    "        self.node_ops = {int(r['Id']): str(r['Op']) for _, r in df_nodes.iterrows()}\n",
    "        self.node_sizes = {int(r['Id']): int(r['Size']) if not pd.isna(r['Size']) else 0 for _, r in df_nodes.iterrows()}\n",
    "        self.node_delta = {int(r['Id']): memory_delta(r) for _, r in df_nodes.iterrows()}\n",
    "        \n",
    "        self.num_nodes = len(self.node_delta)\n",
    "        self.all_nodes = set(self.node_delta.keys())\n",
    "        \n",
    "        # Precompute dependency sets\n",
    "        self.pred_set = {nid: set(preds) for nid, preds in pred.items()}\n",
    "        self.succ_set = {nid: set(succs) for nid, succs in succ.items()}\n",
    "\n",
    "        # Precompute node type sets\n",
    "        self.alloc_nodes = {nid for nid, op in self.node_ops.items() if op == 'ALLOC'}\n",
    "        self.free_nodes = {nid for nid, op in self.node_ops.items() if op == 'FREE'}\n",
    "        self.other_nodes = self.all_nodes - self.alloc_nodes - self.free_nodes\n",
    "        \n",
    "        # Precompute criticality scores\n",
    "        self.criticality_score = self._compute_criticality_scores()\n",
    "        \n",
    "    def _compute_criticality_scores(self):\n",
    "        \"\"\"Compute critical path scores (based on longest path).\"\"\"\n",
    "        score = {nid: 0 for nid in self.all_nodes}\n",
    "        visited = set()\n",
    "        \n",
    "        def dfs(nid):\n",
    "            if nid in visited:\n",
    "                return score[nid]\n",
    "            max_child_score = 0\n",
    "            for neighbor in self.succ.get(nid, []):\n",
    "                child_score = dfs(neighbor)\n",
    "                if child_score > max_child_score:\n",
    "                    max_child_score = child_score\n",
    "            # Assume unit weight per node; score = longest path length from this node\n",
    "            score[nid] = 1 + max_child_score\n",
    "            visited.add(nid)\n",
    "            return score[nid]\n",
    "            \n",
    "        for nid in self.all_nodes:\n",
    "            if nid not in visited:\n",
    "                dfs(nid)\n",
    "                \n",
    "        return score\n",
    "    \n",
    "    def _calculate_priority(self, node, current_memory, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculate node priority combining criticality and memory impact.\"\"\"\n",
    "        # Normalized criticality score ([0, 100])\n",
    "        norm_criticality = (self.criticality_score[node] / self.num_nodes) * 100.0\n",
    "        \n",
    "        # Memory impact score\n",
    "        op = self.node_ops.get(node, '')\n",
    "        delta_val = self.node_delta.get(node, 0)\n",
    "        \n",
    "        if op == 'FREE':\n",
    "            # Higher score for freeing more memory\n",
    "            memory_score = 100 + min(100, abs(delta_val) / 1024)  # Assume 1KB unit, cap at 100\n",
    "        elif op == 'ALLOC':\n",
    "            # Lower allocation size → higher score\n",
    "            memory_score = 50 - min(50, abs(delta_val) / 1024)\n",
    "        else:\n",
    "            memory_score = 50\n",
    "            \n",
    "        # Adjust based on current memory pressure\n",
    "        memory_pressure = current_memory / (max(1, self._get_max_possible_memory()) + 1)\n",
    "        if op == 'FREE':\n",
    "            memory_score *= (1 + memory_pressure)\n",
    "        elif op == 'ALLOC':\n",
    "            memory_score *= (1 - memory_pressure)\n",
    "            \n",
    "        # Combined priority\n",
    "        return alpha * norm_criticality + beta * memory_score\n",
    "    \n",
    "    def _get_max_possible_memory(self):\n",
    "        return sum(d for d in self.node_delta.values() if d > 0)\n",
    "    \n",
    "    def _compute_peak_memory(self, schedule):\n",
    "        current_mem, peak = 0, 0\n",
    "        for node in schedule:\n",
    "            current_mem += self.node_delta.get(node, 0)\n",
    "            current_mem = max(0, current_mem)\n",
    "            if current_mem > peak:\n",
    "                peak = current_mem\n",
    "        return peak\n",
    "    \n",
    "    def _is_valid_schedule(self, schedule):\n",
    "        \"\"\"Check if the schedule respects all dependencies.\"\"\"\n",
    "        if len(set(schedule)) != self.num_nodes:\n",
    "            return False\n",
    "            \n",
    "        position = {}\n",
    "        for idx, node in enumerate(schedule):\n",
    "            for pred_node in self.pred_set.get(node, set()):\n",
    "                if pred_node not in position:\n",
    "                    return False\n",
    "                if position[pred_node] >= idx:\n",
    "                    return False\n",
    "            position[node] = idx\n",
    "        return True\n",
    "    \n",
    "    # ---------- Stage 1: Critical-Path & Memory-Aware Greedy Construction ----------\n",
    "    def cp_memory_aware_construction(self) -> List[int]:\n",
    "        \"\"\"Greedy construction using critical-path and memory awareness.\"\"\"\n",
    "        in_degree = {nid: len(self.pred_set.get(nid, set())) for nid in self.all_nodes}\n",
    "        ready = deque([nid for nid, deg in in_degree.items() if deg == 0])\n",
    "        \n",
    "        schedule = []\n",
    "        current_memory = 0\n",
    "        \n",
    "        while ready:\n",
    "            # Select highest-priority node from ready queue\n",
    "            best_node = None\n",
    "            best_score = float('-inf')\n",
    "            \n",
    "            for node in ready:\n",
    "                score = self._calculate_priority(node, current_memory)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_node = node\n",
    "            \n",
    "            if best_node is None:\n",
    "                best_node = ready.popleft()\n",
    "            else:\n",
    "                ready.remove(best_node)\n",
    "                \n",
    "            schedule.append(best_node)\n",
    "            current_memory += self.node_delta.get(best_node, 0)\n",
    "            current_memory = max(0, current_memory)\n",
    "            \n",
    "            for neighbor in self.succ_set.get(best_node, set()):\n",
    "                in_degree[neighbor] -= 1\n",
    "                if in_degree[neighbor] == 0:\n",
    "                    ready.append(neighbor)\n",
    "                    \n",
    "        return schedule\n",
    "    \n",
    "    # ---------- Stage 2: Simulated Annealing with Heuristic Neighbor Generation ----------\n",
    "    def _generate_heuristic_neighbor(self, schedule):\n",
    "        \"\"\"Generate a heuristic neighbor by swapping promising node pairs.\"\"\"\n",
    "        schedule_copy = schedule.copy()\n",
    "        n = len(schedule_copy)\n",
    "        if n <= 1:\n",
    "            return schedule_copy\n",
    "            \n",
    "        # Try to find good swap candidates\n",
    "        swap_candidates = []\n",
    "        # 1. Prefer FREE ↔ ALLOC swaps\n",
    "        for i in range(n):\n",
    "            if schedule_copy[i] in self.free_nodes:\n",
    "                for j in range(n):\n",
    "                    if i != j and schedule_copy[j] in self.alloc_nodes:\n",
    "                        swap_candidates.append((i, j))\n",
    "                        \n",
    "        # 2. If none, try FREE ↔ OTHER\n",
    "        if not swap_candidates:\n",
    "            for i in range(n):\n",
    "                if schedule_copy[i] in self.free_nodes:\n",
    "                    for j in range(n):\n",
    "                        if i != j and schedule_copy[j] in self.other_nodes:\n",
    "                            swap_candidates.append((i, j))\n",
    "        \n",
    "        # 3. Fallback: random swap\n",
    "        if not swap_candidates:\n",
    "            i, j = random.sample(range(n), 2)\n",
    "            swap_candidates.append((i, j))\n",
    "            \n",
    "        # Randomly pick one candidate and swap\n",
    "        i, j = random.choice(swap_candidates)\n",
    "        schedule_copy[i], schedule_copy[j] = schedule_copy[j], schedule_copy[i]\n",
    "        \n",
    "        return schedule_copy\n",
    "        \n",
    "    def simulated_annealing_optimization(self, initial_schedule, initial_temp=1000, cooling_rate=0.95, iterations=800):\n",
    "        \"\"\"Simulated annealing optimization.\"\"\"\n",
    "        current_schedule = initial_schedule.copy()\n",
    "        current_peak = self._compute_peak_memory(current_schedule)\n",
    "        best_schedule = current_schedule.copy()\n",
    "        best_peak = current_peak\n",
    "        \n",
    "        temp = initial_temp\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            neighbor = self._generate_heuristic_neighbor(current_schedule)\n",
    "            if not self._is_valid_schedule(neighbor):\n",
    "                continue\n",
    "                \n",
    "            neighbor_peak = self._compute_peak_memory(neighbor)\n",
    "            \n",
    "            if neighbor_peak < current_peak:\n",
    "                current_schedule, current_peak = neighbor, neighbor_peak\n",
    "                if neighbor_peak < best_peak:\n",
    "                    best_schedule, best_peak = neighbor, neighbor_peak\n",
    "            else:\n",
    "                acceptance_prob = np.exp(-(neighbor_peak - current_peak) / temp)\n",
    "                if random.random() < acceptance_prob:\n",
    "                    current_schedule, current_peak = neighbor, neighbor_peak\n",
    "                    \n",
    "            temp *= cooling_rate\n",
    "            \n",
    "        return best_schedule\n",
    "    \n",
    "    # ---------- Main Optimization Pipeline ----------\n",
    "    def innovative_optimization(self):\n",
    "        \"\"\"Main two-stage innovative optimization algorithm.\"\"\"\n",
    "        # Step 1: Fast construction of high-quality initial solution\n",
    "        initial_schedule = self.cp_memory_aware_construction()\n",
    "        \n",
    "        # For very small graphs, skip refinement\n",
    "        if self.num_nodes < 20:\n",
    "            peak_memory = self._compute_peak_memory(initial_schedule)\n",
    "            return initial_schedule, peak_memory\n",
    "            \n",
    "        # Step 2: Simulated annealing fine-tuning\n",
    "        final_schedule = self.simulated_annealing_optimization(initial_schedule)\n",
    "        \n",
    "        # Compute final peak memory\n",
    "        peak_memory = self._compute_peak_memory(final_schedule)\n",
    "        \n",
    "        return final_schedule, peak_memory\n",
    "\n",
    "# ---------- Main Workflow ----------\n",
    "\n",
    "def run_innovative_scheduler(task_name: str, nodes_fn: str, edges_fn: str):\n",
    "    nodes_path = os.path.join(DATA_DIR, nodes_fn)\n",
    "    edges_path = os.path.join(DATA_DIR, edges_fn)\n",
    "    \n",
    "    if not (os.path.exists(nodes_path) and os.path.exists(edges_path)):\n",
    "        print(f\"[Skip] {task_name}: Missing CSV files\")\n",
    "        return None\n",
    "\n",
    "    df_nodes, succ, pred = load_graph(nodes_path, edges_path)\n",
    "    scheduler = InnovativeMemoryScheduler(df_nodes, succ, pred)\n",
    "    \n",
    "    print(f\"[{task_name}] Starting innovative optimization...\")\n",
    "    schedule, peak_memory = scheduler.innovative_optimization()\n",
    "    \n",
    "    if not scheduler._is_valid_schedule(schedule):\n",
    "        print(f\"[{task_name}] Warning: Generated schedule is invalid; falling back to greedy result\")\n",
    "        schedule = scheduler.cp_memory_aware_construction()\n",
    "        peak_memory = scheduler._compute_peak_memory(schedule)\n",
    "    \n",
    "    out_sched = os.path.join(OUT_DIR, f\"{task_name}_innovative_schedule.txt\")\n",
    "    with open(out_sched, \"w\", encoding=\"utf-8\") as f:\n",
    "        for nid in schedule:\n",
    "            f.write(str(nid) + \"\\n\")\n",
    "    \n",
    "    metrics = {\n",
    "        \"task\": task_name,\n",
    "        \"num_nodes\": len(df_nodes),\n",
    "        \"peak_memory\": peak_memory,\n",
    "        \"algorithm\": \"innovative_two_stage\"\n",
    "    }\n",
    "    \n",
    "    metrics_path = os.path.join(OUT_DIR, f\"{task_name}_innovative_metrics.json\")\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"[{task_name}] Done! Nodes={len(df_nodes)} PeakMemory={peak_memory}\")\n",
    "    return metrics\n",
    "\n",
    "def discover_tasks(data_dir: str) -> List[Tuple[str, str, str]]:\n",
    "    files = os.listdir(data_dir)\n",
    "    nodes = {f[:-10] for f in files if f.endswith(\"_Nodes.csv\")}\n",
    "    edges = {f[:-10] for f in files if f.endswith(\"_Edges.csv\")}\n",
    "    bases = sorted(nodes & edges)\n",
    "    return [(b, f\"{b}_Nodes.csv\", f\"{b}_Edges.csv\") for b in bases]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    candidates = discover_tasks(DATA_DIR)\n",
    "    results = []\n",
    "    \n",
    "    print(os.path.abspath(os.path.curdir))\n",
    "    print(\"candidates:\", candidates)\n",
    "    for task_name, nodes_file, edges_file in candidates:\n",
    "        try:\n",
    "            starttime = time.time()\n",
    "            result = run_innovative_scheduler(task_name, nodes_file, edges_file)\n",
    "            endtime = time.time()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "            print(\"use time:\", endtime - starttime)\n",
    "            print(\"results:\", results)\n",
    "        except Exception as e:\n",
    "            print(f\"[{task_name}] Error: {str(e)}\")\n",
    "            results.append({\"task\": task_name, \"error\": str(e)})\n",
    "    print(\"results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259286d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2d327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb5fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Matmul_Case0] 节点数=4160 峰值驻留=131328 Makespan=148954\n",
      "[FlashAttention_Case0] 节点数=1716 峰值驻留=42248 Makespan=58370\n",
      "[FlashAttention_Case1] 节点数=6952 峰值驻留=171664 Makespan=204923\n",
      "[Conv_Case1] 节点数=36086 峰值驻留=476790 Makespan=908159\n",
      "[Conv_Case0] 节点数=2580 峰值驻留=62778 Makespan=471605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'task': 'Matmul_Case0',\n",
       "  'num_nodes': 4160,\n",
       "  'peak_memory': 131328,\n",
       "  'makespan_cycles': 148954},\n",
       " {'task': 'FlashAttention_Case0',\n",
       "  'num_nodes': 1716,\n",
       "  'peak_memory': 42248,\n",
       "  'makespan_cycles': 58370},\n",
       " {'task': 'FlashAttention_Case1',\n",
       "  'num_nodes': 6952,\n",
       "  'peak_memory': 171664,\n",
       "  'makespan_cycles': 204923},\n",
       " {'task': 'Conv_Case1',\n",
       "  'num_nodes': 36086,\n",
       "  'peak_memory': 476790,\n",
       "  'makespan_cycles': 908159},\n",
       " {'task': 'Conv_Case0',\n",
       "  'num_nodes': 2580,\n",
       "  'peak_memory': 62778,\n",
       "  'makespan_cycles': 471605}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, heapq, json, pandas as pd, collections\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "DATA_DIR = r\"./CSV版本\"\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"Attachment\", \"Problem1\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "\n",
    "def parse_bufs(x):\n",
    "    \"\"\"Parse 'Bufs' column to list[int], robust to '[1,2]' / '1,2' / None.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x)\n",
    "    nums, cur = [], ''\n",
    "    for ch in s:\n",
    "        if ch.isdigit():\n",
    "            cur += ch\n",
    "        else:\n",
    "            if cur:\n",
    "                nums.append(int(cur))\n",
    "                cur = ''\n",
    "    if cur:\n",
    "        nums.append(int(cur))\n",
    "    return nums\n",
    "\n",
    "def load_graph(nodes_csv: str, edges_csv: str):\n",
    "    \"\"\"Load nodes & edges; build successor and predecessor lists.\"\"\"\n",
    "    df_nodes = pd.read_csv(nodes_csv)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    need_cols = ['Id','Op','BufId','Size','Type','Cycles','Pipe','Bufs']\n",
    "    for c in need_cols:\n",
    "        if c not in df_nodes.columns:\n",
    "            df_nodes[c] = None\n",
    "\n",
    "    # Normalize data types\n",
    "    df_nodes['Id'] = pd.to_numeric(df_nodes['Id'], errors='coerce').astype(int)\n",
    "    for c in ['BufId','Size','Cycles']:\n",
    "        df_nodes[c] = pd.to_numeric(df_nodes[c], errors='coerce')\n",
    "    df_nodes['BufsList'] = df_nodes['Bufs'].apply(parse_bufs)\n",
    "\n",
    "    # Load edges and normalize\n",
    "    df_edges = pd.read_csv(edges_csv)\n",
    "    # Handle case-insensitive or variant column names (e.g., StartNodeId / startnodeid)\n",
    "    cols = {c.lower(): c for c in df_edges.columns}\n",
    "    if 'startnodeid' not in cols or 'endnodeid' not in cols:\n",
    "        raise ValueError(\"Edges CSV must contain columns: StartNodeId, EndNodeId\")\n",
    "    s_col, e_col = cols['startnodeid'], cols['endnodeid']\n",
    "    df_edges[s_col] = pd.to_numeric(df_edges[s_col], errors='coerce').astype(int)\n",
    "    df_edges[e_col] = pd.to_numeric(df_edges[e_col], errors='coerce').astype(int)\n",
    "\n",
    "    # Build graph\n",
    "    node_ids = set(df_nodes['Id'].tolist())\n",
    "    succ = {int(nid): [] for nid in node_ids}\n",
    "    pred = {int(nid): [] for nid in node_ids}\n",
    "    for _, e in df_edges.iterrows():\n",
    "        u = int(e[s_col])\n",
    "        v = int(e[e_col])\n",
    "        # Skip edges referencing nodes not in df_nodes\n",
    "        if u not in node_ids or v not in node_ids:\n",
    "            continue\n",
    "        succ[u].append(v)\n",
    "        pred[v].append(u)\n",
    "    return df_nodes, succ, pred\n",
    "\n",
    "def memory_delta(row) -> int:\n",
    "    \"\"\"Δ(v): +Size for ALLOC, -Size for FREE, 0 otherwise.\"\"\"\n",
    "    op = str(row['Op'])\n",
    "    if op == 'ALLOC':\n",
    "        return int(row['Size']) if not pd.isna(row['Size']) else 0\n",
    "    if op == 'FREE':\n",
    "        return -int(row['Size']) if not pd.isna(row['Size']) else 0\n",
    "    return 0\n",
    "\n",
    "# ---------- Problem 1: Memory-First List Scheduling (MF-LS) ----------\n",
    "\n",
    "def mf_ls_schedule(df_nodes: pd.DataFrame,\n",
    "                   succ: Dict[int, List[int]],\n",
    "                   pred: Dict[int, List[int]]) -> Tuple[List[int], int, List[Tuple[int,int]]]:\n",
    "    delta = {int(r['Id']): memory_delta(r) for _, r in df_nodes.iterrows()}\n",
    "    op_map = {int(r['Id']): str(r['Op']) for _, r in df_nodes.iterrows()}\n",
    "\n",
    "    # ✅ FIX: Use set().union(...) to avoid type errors between list and set\n",
    "    nodes = set().union(delta.keys(), succ.keys(), pred.keys())\n",
    "\n",
    "    # Ensure indeg covers all nodes from df_nodes\n",
    "    indeg = {nid: 0 for nid in nodes}\n",
    "    for v, preds in pred.items():\n",
    "        indeg[v] = len(preds)\n",
    "\n",
    "    def pri(nid: int):\n",
    "        op = op_map.get(nid, '')\n",
    "        dz = delta.get(nid, 0)\n",
    "        if op == 'FREE':\n",
    "            return (0, -abs(dz), nid)  # Prioritize FREE; larger size → higher priority\n",
    "        elif dz == 0:\n",
    "            return (1, 0, nid)         # Neutral operations\n",
    "        else:\n",
    "            return (2, abs(dz), nid)   # ALLOC last; smaller size preferred\n",
    "\n",
    "    heap = []\n",
    "    for nid, d in indeg.items():\n",
    "        if d == 0:\n",
    "            heapq.heappush(heap, (pri(nid), nid))\n",
    "\n",
    "    schedule, mem_series = [], []\n",
    "    cur_mem, peak = 0, 0\n",
    "    visited = 0\n",
    "    while heap:\n",
    "        _, nid = heapq.heappop(heap)\n",
    "        schedule.append(nid)\n",
    "        visited += 1\n",
    "        cur_mem += delta.get(nid, 0)\n",
    "        if cur_mem < 0:  # Prevent negative memory\n",
    "            cur_mem = 0\n",
    "        peak = max(peak, cur_mem)\n",
    "        mem_series.append((nid, cur_mem))\n",
    "        for v in succ.get(nid, []):\n",
    "            indeg[v] -= 1\n",
    "            if indeg[v] == 0:\n",
    "                heapq.heappush(heap, (pri(v), v))\n",
    "\n",
    "    if visited != len(df_nodes):\n",
    "        missing = set(df_nodes['Id']) - set(schedule)\n",
    "        raise RuntimeError(f\"Graph not fully scheduled (possible cycle or isolated nodes not added to ready set). Missing {len(missing)} nodes.\")\n",
    "    return schedule, peak, mem_series\n",
    "\n",
    "# ---------- Optional: Makespan Calculation ----------\n",
    "def compute_makespan(df_nodes: pd.DataFrame, succ: Dict[int, List[int]], schedule: List[int]) -> int:\n",
    "    duration, resource = {}, {}\n",
    "    for _, r in df_nodes.iterrows():\n",
    "        nid = int(r['Id'])\n",
    "        cyc = int(r['Cycles']) if not pd.isna(r['Cycles']) else 0\n",
    "        if str(r['Op']) in ('ALLOC', 'FREE'):\n",
    "            cyc = 0\n",
    "        duration[nid] = max(0, cyc)\n",
    "        resource[nid] = str(r['Pipe']) if not pd.isna(r['Pipe']) else 'MGMT'\n",
    "\n",
    "    preds = {int(r['Id']): [] for _, r in df_nodes.iterrows()}\n",
    "    for u, vs in succ.items():\n",
    "        for v in vs:\n",
    "            preds.setdefault(v, []).append(u)\n",
    "            preds.setdefault(u, preds.get(u, []))\n",
    "\n",
    "    res_end = collections.defaultdict(int)\n",
    "    start, end = {}, {}\n",
    "    for nid in schedule:\n",
    "        est = max([0] + [end.get(p, 0) for p in preds.get(nid, [])])\n",
    "        est = max(est, res_end[resource[nid]])\n",
    "        start[nid] = est\n",
    "        end[nid] = est + duration[nid]\n",
    "        res_end[resource[nid]] = end[nid]\n",
    "    return max(end.values()) if end else 0\n",
    "\n",
    "# ---------- Main Workflow (Callable from Notebook) ----------\n",
    "\n",
    "def run_task(task_name: str, nodes_fn: str, edges_fn: str):\n",
    "    nodes_path = os.path.join(DATA_DIR, nodes_fn)\n",
    "    edges_path = os.path.join(DATA_DIR, edges_fn)\n",
    "    if not (os.path.exists(nodes_path) and os.path.exists(edges_path)):\n",
    "        print(f\"[Skip] {task_name}: Missing CSV files -> {nodes_path} or {edges_path}\")\n",
    "        return None\n",
    "\n",
    "    df_nodes, succ, pred = load_graph(nodes_path, edges_path)\n",
    "    schedule, peak, mem_series = mf_ls_schedule(df_nodes, succ, pred)\n",
    "    makespan = compute_makespan(df_nodes, succ, schedule)  # Optional\n",
    "\n",
    "    # Save schedule\n",
    "    out_sched = os.path.join(OUT_DIR, f\"{task_name}_schedule.txt\")\n",
    "    with open(out_sched, \"w\", encoding=\"utf-8\") as f:\n",
    "        for nid in schedule:\n",
    "            f.write(str(nid) + \"\\n\")\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"task\": task_name,\n",
    "        \"num_nodes\": int(len(df_nodes)),\n",
    "        \"peak_memory\": int(peak),\n",
    "        \"makespan_cycles\": int(makespan)\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, f\"{task_name}_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[{task_name}] Nodes={metrics['num_nodes']} PeakMemory={metrics['peak_memory']} Makespan={metrics['makespan_cycles']}\")\n",
    "    return metrics\n",
    "\n",
    "# ---------- Run All Tasks ----------\n",
    "candidates = [\n",
    "    (\"Matmul_Case0\", \"Matmul_Case0_Nodes.csv\", \"Matmul_Case0_Edges.csv\"),\n",
    "    (\"FlashAttention_Case0\", \"FlashAttention_Case0_Nodes.csv\", \"FlashAttention_Case0_Edges.csv\"),\n",
    "    (\"FlashAttention_Case1\", \"FlashAttention_Case1_Nodes.csv\", \"FlashAttention_Case1_Edges.csv\"),\n",
    "    (\"Conv_Case1\", \"Conv_Case1_Nodes.csv\", \"Conv_Case1_Edges.csv\"),\n",
    "    (\"Conv_Case0\", \"Conv_Case0_Nodes.csv\", \"Conv_Case0_Edges.csv\"),  # Will be skipped if Edges missing\n",
    "]\n",
    "\n",
    "results = []\n",
    "for t, n, e in candidates:\n",
    "    res = run_task(t, n, e)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a21b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据预处理...\n",
      "预处理完成，结果保存到以下CSV文件：\n",
      "./CSV版本\\Matmul_Case0_node_attributes.csv\n",
      "./CSV版本\\Matmul_Case0_in_degree.csv\n",
      "./CSV版本\\Matmul_Case0_buf_lifecycle.csv\n",
      "\n",
      "开始生成最终修复版调度序列...\n",
      "\n",
      "最终调度完成！\n",
      "总节点数：4160 | 已调度节点数：4160 | 未调度节点数：0\n",
      "最大缓存驻留容量: 27904\n",
      "最终调度序列已保存至: ./CSV版本\\Matmul_Case0_schedule_final.csv\n",
      "\n",
      "调度序列预览（前20个节点）:\n",
      "[0, 5, 6, 14, 15, 23, 24, 32, 33, 41, 42, 50, 51, 59, 60, 68, 69, 74, 77, 78]\n",
      "\n",
      "调度序列预览（后20个节点）:\n",
      "[2933, 2935, 4035, 2934, 2936, 3146, 2937, 4156, 4157, 2938, 2940, 4038, 2939, 2941, 3149, 2942, 4158, 4159, 2943, 4143]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory path for input files\n",
    "file_dir = r\"./CSV版本\"\n",
    "\n",
    "# --------------------------\n",
    "# Part 1: Data Preprocessing (unchanged)\n",
    "# --------------------------\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Load node and edge files\n",
    "nodes_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_Nodes.csv')\n",
    "edges_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_Edges.csv')\n",
    "\n",
    "# 1. Build node attribute table (keep Cycles field)\n",
    "node_attributes = {}\n",
    "for _, row in nodes_df.iterrows():\n",
    "    node_id = int(row['Id'])\n",
    "    op_type = row['Op']\n",
    "    buf_id = int(row['BufId']) if pd.notna(row['BufId']) else None\n",
    "    size = int(row['Size']) if pd.notna(row['Size']) else None\n",
    "    cache_type = row['Type'] if pd.notna(row['Type']) else None\n",
    "    cycles = int(row['Cycles']) if pd.notna(row['Cycles']) else 0\n",
    "    node_attributes[node_id] = {\n",
    "        'id': node_id,\n",
    "        'op': op_type,\n",
    "        'buf_id': buf_id,\n",
    "        'size': size,\n",
    "        'cache_type': cache_type,\n",
    "        'cycles': cycles,\n",
    "        'predecessors': [],\n",
    "        'successors': []\n",
    "    }\n",
    "\n",
    "# 2. Process edge information (unchanged)\n",
    "in_degree = defaultdict(int)\n",
    "for _, row in edges_df.iterrows():\n",
    "    src = int(row['StartNodeId'])\n",
    "    dst = int(row['EndNodeId'])\n",
    "    node_attributes[dst]['predecessors'].append(src)\n",
    "    node_attributes[src]['successors'].append(dst)\n",
    "    in_degree[dst] += 1\n",
    "    if src not in in_degree:\n",
    "        in_degree[src] = 0\n",
    "\n",
    "# 3. Build buffer lifecycle mapping (unchanged)\n",
    "buf_lifecycle = defaultdict(dict)\n",
    "for node_id, attr in node_attributes.items():\n",
    "    if attr['op'] == 'ALLOC' and attr['buf_id'] is not None:\n",
    "        buf_id = attr['buf_id']\n",
    "        buf_lifecycle[buf_id]['alloc'] = node_id\n",
    "        buf_lifecycle[buf_id]['uses'] = []\n",
    "    elif attr['op'] == 'FREE' and attr['buf_id'] is not None:\n",
    "        buf_id = attr['buf_id']\n",
    "        buf_lifecycle[buf_id]['free'] = node_id\n",
    "\n",
    "# Populate usage nodes (unchanged)\n",
    "for buf_id, lifecycle in buf_lifecycle.items():\n",
    "    if 'alloc' not in lifecycle or 'free' not in lifecycle:\n",
    "        continue\n",
    "    alloc_id = lifecycle['alloc']\n",
    "    free_id = lifecycle['free']\n",
    "    visited = set()\n",
    "    queue = [alloc_id]\n",
    "    while queue:\n",
    "        current = queue.pop(0)\n",
    "        if current == free_id:\n",
    "            break\n",
    "        if current in visited:\n",
    "            continue\n",
    "        visited.add(current)\n",
    "        if current != alloc_id:\n",
    "            lifecycle['uses'].append(current)\n",
    "        for succ in node_attributes[current]['successors']:\n",
    "            if succ not in visited:\n",
    "                queue.append(succ)\n",
    "\n",
    "# 4. Save preprocessing results (unchanged)\n",
    "node_attr_df = pd.DataFrame(node_attributes.values())\n",
    "node_attr_df.to_csv(f'{file_dir}\\\\Matmul_Case0_node_attributes.csv', index=False)\n",
    "\n",
    "in_degree_df = pd.DataFrame(list(in_degree.items()), columns=['NodeId', 'InDegree'])\n",
    "in_degree_df.to_csv(f'{file_dir}\\\\Matmul_Case0_in_degree.csv', index=False)\n",
    "\n",
    "buf_lifecycle_list = []\n",
    "for buf_id, data in buf_lifecycle.items():\n",
    "    record = {\n",
    "        'BufId': buf_id,\n",
    "        'AllocNode': data['alloc'],\n",
    "        'UseNodes': ','.join(str(x) for x in data['uses']) if data['uses'] else '',\n",
    "        'FreeNode': data['free']\n",
    "    }\n",
    "    buf_lifecycle_list.append(record)\n",
    "buf_lifecycle_df = pd.DataFrame(buf_lifecycle_list)\n",
    "buf_lifecycle_df.to_csv(f'{file_dir}\\\\Matmul_Case0_buf_lifecycle.csv', index=False)\n",
    "\n",
    "print('Preprocessing completed. Results saved to the following CSV files:')\n",
    "print(f'{file_dir}\\\\Matmul_Case0_node_attributes.csv')\n",
    "print(f'{file_dir}\\\\Matmul_Case0_in_degree.csv')\n",
    "print(f'{file_dir}\\\\Matmul_Case0_buf_lifecycle.csv')\n",
    "\n",
    "# --------------------------\n",
    "# Part 2: Final Fixed Scheduling Sequence Generation\n",
    "# --------------------------\n",
    "print(\"\\nGenerating final fixed scheduling sequence...\")\n",
    "\n",
    "# 1. Load preprocessed data (unchanged)\n",
    "node_attr_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_node_attributes.csv')\n",
    "node_attributes = {}\n",
    "for _, row in node_attr_df.iterrows():\n",
    "    node_id = int(row['id'])\n",
    "    node_attributes[node_id] = {\n",
    "        'id': node_id,\n",
    "        'op': row['op'],\n",
    "        'buf_id': int(row['buf_id']) if pd.notna(row['buf_id']) else None,\n",
    "        'size': int(row['size']) if pd.notna(row['size']) else None,\n",
    "        'cache_type': row['cache_type'] if pd.notna(row['cache_type']) else None,\n",
    "        'cycles': int(row['cycles']) if pd.notna(row['cycles']) else 0,\n",
    "        'predecessors': eval(row['predecessors']),\n",
    "        'successors': eval(row['successors'])\n",
    "    }\n",
    "\n",
    "in_degree_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_in_degree.csv')\n",
    "in_degree = {int(row['NodeId']): int(row['InDegree']) for _, row in in_degree_df.iterrows()}\n",
    "\n",
    "buf_lifecycle_df = pd.read_csv(f'{file_dir}\\\\Matmul_Case0_buf_lifecycle.csv')\n",
    "buf_lifecycle = {}\n",
    "for _, row in buf_lifecycle_df.iterrows():\n",
    "    buf_id = int(row['BufId'])\n",
    "    use_nodes_str = str(row['UseNodes']) if pd.notna(row['UseNodes']) else ''\n",
    "    uses = []\n",
    "    if use_nodes_str and use_nodes_str != 'nan' and use_nodes_str != '0':\n",
    "        uses = list(map(int, use_nodes_str.split(',')))\n",
    "    buf_lifecycle[buf_id] = {\n",
    "        'alloc': int(row['AllocNode']),\n",
    "        'uses': uses,\n",
    "        'free': int(row['FreeNode'])\n",
    "    }\n",
    "\n",
    "# 2. Initialize scheduling data structures (with node status tracking)\n",
    "schedule = []\n",
    "candidate_heap = []\n",
    "current_cache = 0\n",
    "max_cache = 0\n",
    "buf_in_use = set()\n",
    "current_step = 0\n",
    "\n",
    "# New 1: Node status tracking (to avoid duplicate scheduling or re-adding)\n",
    "node_status = {node_id: 'pending' for node_id in node_attributes}  # pending/processing/completed\n",
    "retry_count = defaultdict(int)  # Retry count per node (max 3 times)\n",
    "MAX_RETRY = 3  # Maximum retries before marking a node as invalid\n",
    "\n",
    "# L0 cache progress tracking (unchanged)\n",
    "l0_progress = {\n",
    "    'L0A': {'in_use': False, 'current_op': None, 'progress': 0},\n",
    "    'L0B': {'in_use': False, 'current_op': None, 'progress': 0},\n",
    "    'L0C': {'in_use': False, 'current_op': None, 'progress': 0}\n",
    "}\n",
    "\n",
    "# Future demand queue parameter (unchanged)\n",
    "K = 15\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Core Fix 1: Multi-dimensional Priority Calculation (unchanged)\n",
    "# --------------------------\n",
    "def calculate_priority(node, current_step):\n",
    "    priority = 0\n",
    "    buf_id = node['buf_id']\n",
    "    op_type = node['op']\n",
    "\n",
    "    if op_type == 'FREE' and buf_id is not None:\n",
    "        buf_size = node['size'] if (node['size'] is not None and buf_id in buf_lifecycle) else 0\n",
    "        priority += (100 + buf_size) * 0.4\n",
    "\n",
    "    elif op_type in ['ALLOC', 'COPY_IN', 'COPY_OUT', 'MMAD', 'SOFTMAX'] and buf_id in buf_lifecycle:\n",
    "        lifecycle = buf_lifecycle[buf_id]\n",
    "        if 'free' in lifecycle and 'alloc' in lifecycle:\n",
    "            total_lifecycle = lifecycle['free'] - lifecycle['alloc']\n",
    "            if total_lifecycle > 0:\n",
    "                current_reside = current_step - lifecycle['alloc']\n",
    "                lifecycle_ratio = min(current_reside / total_lifecycle, 1.0)\n",
    "                priority += lifecycle_ratio * 100 * 0.3\n",
    "\n",
    "    if op_type not in ['ALLOC', 'FREE']:\n",
    "        priority += node['cycles'] * 0.2\n",
    "\n",
    "    if node['cache_type'] in ['L0A', 'L0B', 'L0C']:\n",
    "        priority += 50 * 0.1\n",
    "\n",
    "    return priority\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Core Fix 2: Conflict Prediction Function (unchanged)\n",
    "# --------------------------\n",
    "def predict_cache_conflict(future_alloc_list, cache_type, current_cache_used):\n",
    "    cache_capacity_map = {'L1': 4096, 'UB': 1024, 'L0A': 256, 'L0B': 256, 'L0C': 512}\n",
    "    if cache_type not in cache_capacity_map:\n",
    "        return False\n",
    "    cache_capacity = cache_capacity_map[cache_type]\n",
    "    future_total = sum(alloc['size'] for alloc in future_alloc_list if alloc['cache_type'] == cache_type)\n",
    "    return (current_cache_used + future_total) > cache_capacity\n",
    "\n",
    "\n",
    "# Initialize candidate heap (only add 'pending' nodes)\n",
    "for node_id, degree in in_degree.items():\n",
    "    if degree == 0 and node_status[node_id] == 'pending':\n",
    "        node = node_attributes[node_id]\n",
    "        priority = calculate_priority(node, current_step=0)\n",
    "        heapq.heappush(candidate_heap, (-priority, node_id))\n",
    "        node_status[node_id] = 'processing'  # Mark as processing to avoid re-adding\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Core Fix 3: Main Scheduling Loop (resolves infinite loop)\n",
    "# --------------------------\n",
    "# Track total number of nodes to detect completion\n",
    "total_nodes = len(node_attributes)\n",
    "while candidate_heap:\n",
    "    # Termination condition 1: All nodes scheduled\n",
    "    if len(schedule) >= total_nodes:\n",
    "        print(f\"Early termination: All {total_nodes} nodes have been scheduled.\")\n",
    "        break\n",
    "\n",
    "    current_step = len(schedule)\n",
    "\n",
    "    # Step 1: Update future demand queue (only consider pending/processing nodes)\n",
    "    temp_heap = candidate_heap.copy()\n",
    "    future_alloc_queue = []\n",
    "    count = 0\n",
    "    while temp_heap and count < K:\n",
    "        try:\n",
    "            neg_prio, node_id = heapq.heappop(temp_heap)\n",
    "        except IndexError:\n",
    "            break\n",
    "        if node_status[node_id] != 'completed':  # Only consider unscheduled nodes\n",
    "            node = node_attributes[node_id]\n",
    "            if node['op'] == 'ALLOC' and node['cache_type'] is not None and node['buf_id'] is not None:\n",
    "                future_alloc_queue.append({\n",
    "                    'buf_id': node['buf_id'],\n",
    "                    'size': node['size'] if node['size'] is not None else 0,\n",
    "                    'cache_type': node['cache_type']\n",
    "                })\n",
    "        count += 1\n",
    "\n",
    "    # Step 2: Pop highest-priority valid node (skip completed or over-retried nodes)\n",
    "    valid_node = False\n",
    "    while candidate_heap and not valid_node:\n",
    "        try:\n",
    "            neg_prio, current_node_id = heapq.heappop(candidate_heap)\n",
    "        except IndexError:\n",
    "            break\n",
    "        # Skip if already completed\n",
    "        if node_status[current_node_id] == 'completed':\n",
    "            continue\n",
    "        # Skip if retry limit exceeded\n",
    "        if retry_count[current_node_id] > MAX_RETRY:\n",
    "            print(f\"Node {current_node_id} exceeded max retry count ({MAX_RETRY}), marked as invalid.\")\n",
    "            node_status[current_node_id] = 'completed'\n",
    "            continue\n",
    "        # Valid node found\n",
    "        current_node = node_attributes[current_node_id]\n",
    "        valid_node = True\n",
    "\n",
    "    # Termination condition 2: No valid nodes left (heap exhausted)\n",
    "    if not valid_node:\n",
    "        print(f\"Terminated: No valid nodes available for scheduling ({len(schedule)}/{total_nodes} scheduled).\")\n",
    "        break\n",
    "\n",
    "    # Step 3: L0 cache fine-grained scheduling check (with retry limit)\n",
    "    l0_skip = False\n",
    "    if current_node['op'] == 'ALLOC' and current_node['cache_type'] in ['L0A', 'L0B', 'L0C']:\n",
    "        l0_type = current_node['cache_type']\n",
    "        l0_info = l0_progress[l0_type]\n",
    "        buf_id = current_node['buf_id']\n",
    "        if buf_id is None:\n",
    "            retry_count[current_node_id] += 1\n",
    "            heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "            l0_skip = True\n",
    "            continue\n",
    "\n",
    "        if l0_info['in_use']:\n",
    "            if l0_info['progress'] >= 90:\n",
    "                free_node_id = next((n for n in node_attributes if\n",
    "                                     node_attributes[n]['op'] == 'FREE' and\n",
    "                                     node_attributes[n]['buf_id'] == buf_id and\n",
    "                                     node_status[n] == 'pending'), None)  # Only look for pending FREE nodes\n",
    "                if free_node_id and in_degree[free_node_id] == 0:\n",
    "                    free_node = node_attributes[free_node_id]\n",
    "                    free_prio = calculate_priority(free_node, current_step)\n",
    "                    # Execute FREE node\n",
    "                    if buf_id in buf_in_use:\n",
    "                        current_cache -= free_node['size'] if free_node['size'] is not None else 0\n",
    "                        buf_in_use.remove(buf_id)\n",
    "                    schedule.append(free_node_id)\n",
    "                    node_status[free_node_id] = 'completed'  # Mark as completed\n",
    "                    # Reset L0 state\n",
    "                    l0_progress[l0_type]['in_use'] = False\n",
    "                    l0_progress[l0_type]['current_op'] = None\n",
    "                    l0_progress[l0_type]['progress'] = 0\n",
    "                else:\n",
    "                    retry_count[current_node_id] += 1\n",
    "                    heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "                    l0_skip = True\n",
    "            else:\n",
    "                retry_count[current_node_id] += 1\n",
    "                heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "                l0_skip = True\n",
    "        else:\n",
    "            if buf_id in buf_lifecycle:\n",
    "                use_node_id = next((n for n in buf_lifecycle[buf_id].get('uses', [])\n",
    "                                    if node_attributes[n]['op'] == 'MMAD' and\n",
    "                                    node_status[n] == 'pending'), None)  # Only pending MMAD nodes\n",
    "                if use_node_id:\n",
    "                    l0_progress[l0_type]['in_use'] = True\n",
    "                    l0_progress[l0_type]['current_op'] = use_node_id\n",
    "    if l0_skip:\n",
    "        continue\n",
    "\n",
    "    # Step 4: Buffer conflict prediction (with retry limit)\n",
    "    alloc_skip = False\n",
    "    if current_node['op'] == 'ALLOC' and current_node['cache_type'] is not None and current_node['buf_id'] is not None:\n",
    "        cache_type = current_node['cache_type']\n",
    "        buf_id = current_node['buf_id']\n",
    "        buf_size = current_node['size'] if current_node['size'] is not None else 0\n",
    "\n",
    "        # Compute current used capacity for this cache type\n",
    "        current_cache_used = 0\n",
    "        for bid in buf_in_use:\n",
    "            if bid in buf_lifecycle and buf_lifecycle[bid].get('alloc') in node_attributes:\n",
    "                alloc_node = node_attributes[buf_lifecycle[bid]['alloc']]\n",
    "                if alloc_node['cache_type'] == cache_type and alloc_node['size'] is not None:\n",
    "                    current_cache_used += alloc_node['size']\n",
    "\n",
    "        if predict_cache_conflict(future_alloc_queue, cache_type, current_cache_used):\n",
    "            # Find pending FREE nodes that can release space\n",
    "            free_node_candidates = [\n",
    "                n for n in node_attributes\n",
    "                if (node_attributes[n]['op'] == 'FREE' and\n",
    "                    node_attributes[n]['cache_type'] == cache_type and\n",
    "                    node_attributes[n]['buf_id'] is not None and\n",
    "                    in_degree[n] == 0 and\n",
    "                    node_status[n] == 'pending')\n",
    "            ]\n",
    "\n",
    "            if free_node_candidates:\n",
    "                def get_free_size(node_id):\n",
    "                    node = node_attributes[node_id]\n",
    "                    return node['size'] if node['size'] is not None else 0\n",
    "\n",
    "                free_node_id = max(free_node_candidates, key=get_free_size)\n",
    "                free_node = node_attributes[free_node_id]\n",
    "                free_buf_id = free_node['buf_id']\n",
    "\n",
    "                # Release buffer\n",
    "                if free_buf_id in buf_in_use:\n",
    "                    current_cache -= get_free_size(free_node_id)\n",
    "                    buf_in_use.remove(free_buf_id)\n",
    "                    if free_node['cache_type'] in ['L0A', 'L0B', 'L0C']:\n",
    "                        l0_progress[free_node['cache_type']]['in_use'] = False\n",
    "                        l0_progress[free_node['cache_type']]['current_op'] = None\n",
    "                        l0_progress[free_node['cache_type']]['progress'] = 0\n",
    "\n",
    "                # Schedule FREE node\n",
    "                schedule.append(free_node_id)\n",
    "                node_status[free_node_id] = 'completed'\n",
    "                # Re-add current ALLOC node (increment retry)\n",
    "                retry_count[current_node_id] += 1\n",
    "                heapq.heappush(candidate_heap, (neg_prio, current_node_id))\n",
    "                alloc_skip = True\n",
    "    if alloc_skip:\n",
    "        continue\n",
    "\n",
    "    # Step 5: Execute current node (mark as completed immediately)\n",
    "    schedule.append(current_node_id)\n",
    "    node_status[current_node_id] = 'completed'  # Critical: prevent reprocessing\n",
    "    retry_count[current_node_id] = 0  # Reset retry count\n",
    "\n",
    "    # Step 6: Update L0 operation progress (unchanged)\n",
    "    for l0_type in l0_progress:\n",
    "        l0_info = l0_progress[l0_type]\n",
    "        if l0_info['in_use'] and l0_info['current_op'] == current_node_id:\n",
    "            step_progress = 100 / max(current_node['cycles'], 1)\n",
    "            l0_info['progress'] = min(l0_info['progress'] + step_progress, 100)\n",
    "            buf_id = current_node['buf_id']\n",
    "            if buf_id is not None and buf_id in buf_lifecycle and 'free' in buf_lifecycle[buf_id]:\n",
    "                free_node_id = buf_lifecycle[buf_id]['free']\n",
    "                if free_node_id in node_attributes and in_degree[free_node_id] == 0 and node_status[free_node_id] == 'pending':\n",
    "                    free_node = node_attributes[free_node_id]\n",
    "                    free_prio = calculate_priority(free_node, current_step + 1)\n",
    "                    heapq.heappush(candidate_heap, (-free_prio, free_node_id))\n",
    "                    node_status[free_node_id] = 'processing'\n",
    "\n",
    "    # Step 7: Update cache state (unchanged)\n",
    "    buf_id = current_node['buf_id']\n",
    "    if current_node['op'] == 'ALLOC' and buf_id is not None:\n",
    "        if buf_id not in buf_in_use:\n",
    "            buf_size = current_node['size'] if current_node['size'] is not None else 0\n",
    "            current_cache += buf_size\n",
    "            buf_in_use.add(buf_id)\n",
    "\n",
    "    elif current_node['op'] == 'FREE' and buf_id is not None:\n",
    "        if buf_id in buf_in_use:\n",
    "            buf_size = current_node['size'] if current_node['size'] is not None else 0\n",
    "            current_cache -= buf_size\n",
    "            buf_in_use.remove(buf_id)\n",
    "\n",
    "    # Step 8: Update peak memory usage (unchanged)\n",
    "    if current_cache > max_cache:\n",
    "        max_cache = current_cache\n",
    "\n",
    "    # Step 9: Update successor in-degrees (only add pending nodes)\n",
    "    for succ_id in current_node['successors']:\n",
    "        if succ_id not in node_attributes or node_status[succ_id] == 'completed':\n",
    "            continue  # Skip missing or completed successors\n",
    "        in_degree[succ_id] -= 1\n",
    "        if in_degree[succ_id] == 0 and node_status[succ_id] == 'pending':\n",
    "            succ_node = node_attributes[succ_id]\n",
    "            succ_prio = calculate_priority(succ_node, current_step + 1)\n",
    "            heapq.heappush(candidate_heap, (-succ_prio, succ_id))\n",
    "            node_status[succ_id] = 'processing'  # Mark as processing\n",
    "\n",
    "\n",
    "# 4. Save results and output summary (with completeness validation)\n",
    "schedule_df = pd.DataFrame({'NodeId': schedule})\n",
    "schedule_df.to_csv(f'{file_dir}\\\\Matmul_Case0_schedule_final.csv', index=False)\n",
    "\n",
    "# Output scheduling completeness info\n",
    "completed_nodes = set(schedule)\n",
    "missing_nodes = [node_id for node_id in node_attributes if node_id not in completed_nodes]\n",
    "print(f\"\\nScheduling completed!\")\n",
    "print(f\"Total nodes: {total_nodes} | Scheduled: {len(schedule)} | Unscheduled: {len(missing_nodes)}\")\n",
    "if missing_nodes:\n",
    "    print(f\"Unscheduled node IDs (first 10): {missing_nodes[:10]}...\")\n",
    "print(f\"Peak memory footprint: {max_cache}\")\n",
    "print(f\"Final schedule saved to: {file_dir}\\\\Matmul_Case0_schedule_final.csv\")\n",
    "print(\"\\nSchedule preview (first 20 nodes):\")\n",
    "print(schedule[:20] if len(schedule) >= 20 else schedule)\n",
    "print(\"\\nSchedule preview (last 20 nodes):\")\n",
    "print(schedule[-20:] if len(schedule) >= 20 else schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc3625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
